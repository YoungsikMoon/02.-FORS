{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1y7Qj3juicdvoJFFC8u9uYaPR0Lwg0YqU",
      "authorship_tag": "ABX9TyPAPu251PMm4R6w8bbQ0ly1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoungsikMoon/FORS/blob/main/%EC%A0%95%ED%98%B8%EC%84%9D/FORS_%EC%A0%95%ED%98%B8%EC%84%9D_%ED%8C%A8%EC%8A%A4%ED%8A%B8_%EC%9C%84%EC%8A%A4%ED%8D%BC%ED%8C%8C%EC%9D%B8%ED%8A%9C%EB%8B%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-phpfIw7gqEK"
      },
      "outputs": [],
      "source": [
        "# 필수 패키지 설치\n",
        "\n",
        "!pip install datasets>=2.6.1\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install evaluate>=0.30\n",
        "!pip install jiwer\n",
        "!pip install accelerate -U\n",
        "!pip install transformers[torch]\n",
        "!pip install --upgrade bitsandbytes\n",
        "!pip install peft"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### - 최종 전처리된 datasetDict 객체 불러오기\n",
        "- Fast Whisper는 여기서부터 시작"
      ],
      "metadata": {
        "id": "5cCZGPBUJZpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 완료된 datasetDict 객체 디스크(구글 드라이브)에서 불러오기\n",
        "# 이전 과정은 위스퍼 파인튜닝 파일에 있음\n",
        "from datasets import load_from_disk\n",
        "low_call_voices_prepreocessed = load_from_disk(\"/content/drive/MyDrive/whisper/low_call_voice\")"
      ],
      "metadata": {
        "id": "PpmWH2N1JId7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 학습 (Fine-Tuning) 진행\n",
        "- 전처리된 데이터로 학습진행 시작\n",
        "- 여기서부터 GPU/TPU 필수\n",
        "- GPU/TPU 주는 캐글 노트북으로 건너가서 진행\n",
        "    - 하려고 했으나 캐글에 16기가 데이터가 업로드 되지 않음.."
      ],
      "metadata": {
        "id": "TkAvnY4yVCUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Data Collator\n",
        "- 전처리한 데이터를 모델에 입력할 수 있는 PyTorch 텐서 형태로 변환하는 작업\n",
        "- 이 때 패딩이 되지 않은 타겟(labels = 발화데이터)도 같이 패딩처리해줌"
      ],
      "metadata": {
        "id": "HZTwoNuuPRyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "from transformers import WhisperProcessor\n",
        "\n",
        "# 전처리된 데이터를 텐서 형태로 변환\n",
        "# 제일 이해 안가는 코드..\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features : List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        # featurs는 최종 datasetDict 자료 (3개로 이뤄져있음)\n",
        "            # train : input_features, labels\n",
        "            # valid : input_features, labels\n",
        "            # test : input_features, labels\n",
        "\n",
        "        # 인풋 데이터(mel-log로 변환된 오디오 파일)를 토치 텐서로 변환\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "\n",
        "        # input_feaures를 패딩. 이미 패딩 돼 있는데 왜 또하는지는 잘 모르겠음.\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # 라벨 데이터 (정수 인코딩된 발화데이터)를 토치 텐서로 변환\n",
        "        # ex) train이 가진 [\"input_features\"] 컬럼을 딕셔너리 구조를 가진 리스트 안에다가 넣어줌\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        # 라벨 데이터 패딩 적용\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # 패딩 토큰을 -100으로 치환해 loss 계산 과정에서 무시되도록 함\n",
        "        # 이미 패팅 토큰이라는 것 자체가 무시되도록 돼 있을텐데 왜 하는지를 잘 모르겠음\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # 이전 토크나이즈 과정에서 bos 토큰이 추가되었다면 bos 토큰을 잘라냄\n",
        "        # 이 코드는 뭔지 잘 모르겠음..\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "\n",
        "# 데이터 콜레이터 초기화\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ],
      "metadata": {
        "id": "cEMRv4FwVHOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Evaluaion Metrics\n",
        "    - 검증 데이터셋에 사용한 검증 매트릭스 정의\n",
        "    - 한국어의 경우 WER 보다 CER이 더 적합"
      ],
      "metadata": {
        "id": "wPbEtF8bSq5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 패스트 위스퍼에서 이건 사용 못함\n",
        "\n",
        "# import evaluate\n",
        "\n",
        "# metric = evaluate.load('cer')\n",
        "\n",
        "# def compute_metrics(pred):\n",
        "\n",
        "#     # 예측값\n",
        "#     pred_ids = pred.predictions\n",
        "\n",
        "#     # 실제값\n",
        "#     label_ids = pred.label_ids\n",
        "\n",
        "#     # 패딩된 토큰을 올바르게 무시하기 위해 -100을 다시 pad_token으로 치환\n",
        "#     # 밑에 kip_special_tokens=True가 적용될 수 있도록 함\n",
        "#     label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "#     # 정수 인덱스를 실제 묹자열로 디코딩\n",
        "#     # metrics 계산 시 special token들을 빼고 계산하도록 설정\n",
        "#     pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "#     label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "#     # CER 계산\n",
        "#     cer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "#     return {\"cer\": cer}"
      ],
      "metadata": {
        "id": "VI4OZCgtSYXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Load a Pre-Trained Checkpoint (Fast Whisper)\n",
        "- 패스트 위스퍼 사용\n",
        "- 32bit의 Float 가중치 대신 8bit int형 가중치를 사용해 메모리 사용량 감소\n",
        "- LoRA(Low-rank adapters) 적용\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- pre_trained model : 이미 학습된 위스퍼의 기본 모델을 뜻함\n",
        "- tiny, small, medium, large 등의 학습된 모델 종류를 지정해서 모델을 로드"
      ],
      "metadata": {
        "id": "lMx48T3-TF1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration, BitsAndBytesConfig\n",
        "\n",
        "# 8비트 옵션 적용해서 모델 로드\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"Systran/faster-whisper-large-v3\", quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map={\" \":0})\n",
        "\n",
        "# 한국어 고정이기 때문에 언어를 고정해주는 것이 좋음\n",
        "model.generation_config.language = \"korean\"\n",
        "\n",
        "# 한국어 전사 태스크임을 명시\n",
        "# 프로세서에서 명시해줬지만 모델에서도 다시 명시해주는게 좋은 듯\n",
        "model.generation_config.task = \"transcribe\"\n",
        "\n",
        "# 디폴트값인 any는 디코더가 다국어에 맞는 토큰을 자동 생성하도록 함\n",
        "# 한국어만 보면 되므로 None으로 지정해서 좀 더 정확성을 올려줌\n",
        "model.config.forced_decoder_ids = None\n",
        "\n",
        "# 문장 생성 중 억제되는 토큰이 없도록 리스트를 비워줌\n",
        "# 정확한 의미는 모르겠음..\n",
        "# suppress_tokens는 일종의 불용어 사전 같은 용도\n",
        "model.config.suppress_tokens = []"
      ],
      "metadata": {
        "id": "-Uf5bv02TxHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PEFT 적용을 통한 모델 전처리\n",
        "- 가중치를 float32 => int8로 바뀐 것에 대한 전처리 진행"
      ],
      "metadata": {
        "id": "JdmzB-irVIun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "GZl5N8qaVbkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LoRA 어댑터 적용\n",
        "- 파라미터를 1%로 줄여버림"
      ],
      "metadata": {
        "id": "jjPT0CZXVeJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.5, bias=\"none\")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "bKJh4TeeqFM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####4. Define the Training Arguments\n",
        "- 최종 학습을 위한 파라미터 설정\n",
        "- 에포크 횟수, 모델 저장 경로 등등"
      ],
      "metadata": {
        "id": "Xdbiuhg6ULEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 패스트 위스퍼 파라미터 설정\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/Fast_whisper/fast_save_model\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-3,\n",
        "    warmup_steps=50,\n",
        "    num_train_epochs=3,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    per_device_eval_batch_size=8,\n",
        "    generation_max_length=128,\n",
        "    logging_steps=25,\n",
        "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
        "    label_names=[\"labels\"],  # same reason as above\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "from transformers import Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
        "import os\n",
        "\n",
        "class SavePeftModelCallback(TrainerCallback):\n",
        "    def on_save(\n",
        "        self,\n",
        "        args: TrainingArguments,\n",
        "        state: TrainerState,\n",
        "        control: TrainerControl,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
        "\n",
        "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
        "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
        "\n",
        "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
        "        if os.path.exists(pytorch_model_path):\n",
        "            os.remove(pytorch_model_path)\n",
        "        return control\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=low_call_voices_prepreocessed[\"train\"],\n",
        "    eval_dataset=low_call_voices_prepreocessed[\"valid\"],\n",
        "    data_collator=data_collator,\n",
        "    # compute_metrics=compute_metrics, # 이건 사용 못함\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    callbacks=[SavePeftModelCallback],\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ],
      "metadata": {
        "id": "ZNnIhvu7UWbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####5. 학습 진행\n",
        "- 학습 진행\n",
        "- 진행 후 바로 저장\n",
        "- 리소스가 있어야 하지.."
      ],
      "metadata": {
        "id": "db1BJ-8WVepj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "DFHxDSbjVeMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 모델, 프로세서 저장 (토크나이저는 프로세서 안에 있으니까 저장안함)\n",
        "trainer.save_pretrained(\"/content/drive/MyDrive/whisper/save_model/\")\n",
        "processor.save_pretrained(\"/content/drive/MyDrive/whisper/save_model/\")"
      ],
      "metadata": {
        "id": "2Zp2w6OibLPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 평가진행\n",
        "- 위 과정은 train, vaild를 이용한 훈련과 검증 과정\n",
        "- 평가는 Test를 사용해 진행"
      ],
      "metadata": {
        "id": "gXs5cb2WXUY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer\n",
        "\n",
        "peft_model_id = \"smangrul/openai-whisper-large-v2-LORA-colab\"\n",
        "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\n",
        "    peft_config.base_model_name_or_path, quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map=\"auto\"\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "\n"
      ],
      "metadata": {
        "id": "H4_3so_qXa7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import gc\n",
        "import evaluate\n",
        "\n",
        "metric = evaluate.load('cer')\n",
        "\n",
        "eval_dataloader = DataLoader(common_voice[\"test\"], batch_size=8, collate_fn=data_collator)\n",
        "\n",
        "model.eval()\n",
        "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
        "    with torch.cuda.amp.autocast():\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = (\n",
        "                model.generate(\n",
        "                    input_features=batch[\"input_features\"].to(\"cuda\"),\n",
        "                    decoder_input_ids=batch[\"labels\"][:, :4].to(\"cuda\"),\n",
        "                    max_new_tokens=255,\n",
        "                )\n",
        "                .cpu()\n",
        "                .numpy()\n",
        "            )\n",
        "            labels = batch[\"labels\"].cpu().numpy()\n",
        "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "            metric.add_batch(\n",
        "                predictions=decoded_preds,\n",
        "                references=decoded_labels,\n",
        "            )\n",
        "    del generated_tokens, labels, batch\n",
        "    gc.collect()\n",
        "wer = 100 * metric.compute()\n",
        "print(f\"{wer=}\")"
      ],
      "metadata": {
        "id": "zraicfOpbfXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OViyeN76Xrbd"
      }
    }
  ]
}