{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 크롤링 단계\n",
        "## 주요함수\n",
        "- driver.find_element(By.XPATH, ...) html 경로 검색\n",
        "- driver = webdriver.Chrome() : 셀레니움에서 웹창을 통해 정보추출\n",
        "- driver.get('https://www.bomtoon.com/user/login') : 셀레니움 get방식 추출\n",
        "- driver.post('https://www.bomtoon.com/user/login') : 셀레니움 get방식 추출\n",
        "- while_try/except : while문에서 로그인 예외처리\n",
        "- IndexError : except에서 인덱스가 없는 경우 예외처리\n",
        "- with open('경로', 'a/w/r', encoding='utf-8') as f: f.write(str(dic_res)) : a(새로운 내용 추가)/w(쓰기)/r(읽기)\n",
        "- pd.Series : dic 데이터 프레임화\n"
      ],
      "metadata": {
        "id": "AWTjUmFOO0lq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci_mu_21Oceb"
      },
      "outputs": [],
      "source": [
        "# https://www.bomtoon.com/bom/comic/weekly    봄툰 주간 웹툰 주소\n",
        "\n",
        "\n",
        "# 셀레늄 설치 여부\n",
        "!pip list | findstr selenium\n",
        "\n",
        "# 설치 명령\n",
        "!pip install selenium\n",
        "!pip install pyperclip\n",
        "\n",
        "# https://chromedriver.chromium.org/downloads    크롬 버전 확인\n",
        "\n",
        "# https://chromedriver.com/download#stable      웹 드라이브 다운로드 주소 (상시 업데이트 중)\n",
        "# https://storage.googleapis.com/chrome-for-testing-public/123.0.6312.58/win64/chromedriver-win64.zip   버전 123.0.6312.58(공식 빌드) (64비트) 사용"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()   # OS 디렉터리 위치 확인"
      ],
      "metadata": {
        "id": "-s4DDaB_OpRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver   # 동적 코드 크롤링을 위한 가상의 크롬 브라우저 (실제 크롬 브라우저는 아님)\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import pandas as pd\n",
        "import time\n",
        "import pyperclip\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import os\n",
        "\n",
        "\n",
        "driver = webdriver.Chrome()  #웹 드라이버 호출\n",
        "driver.get('https://www.bomtoon.com/user/login')  #툰 로그인 주소 호출\n",
        "nid='om'  # 봄툰 아이디 입력\n",
        "npw='()' # 봄툰 패스워드 입력\n",
        "\n",
        "time.sleep(3) # 동적으로 화면을 크롤링하기 때문에 데이터가 제공되는 시간을 기다리기 위해\n",
        "driver.find_element(By.XPATH,'//*[@id=\"__next\"]/div/div[1]/div[2]/div/div/div/div[4]/input[1]').send_keys(nid)\n",
        "time.sleep(3)\n",
        "driver.find_element(By.XPATH,'//*[@id=\"__next\"]/div/div[1]/div[2]/div/div/div/div[4]/input[2]').send_keys(npw)\n",
        "time.sleep(3)\n",
        "driver.find_element(By.XPATH,'//*[@id=\"__next\"]/div/div[1]/div[2]/div/div/div/div[6]').click()\n",
        "\n",
        "driver.implicitly_wait(10) # 셀레니움 내부적으로 버퍼링 지연을 시켜줌 (로그인 인증 키값을 불러오는 시간을 기다리기 위해)\n",
        "\n",
        "time.sleep(3)\n",
        "co=driver.get_cookies() # 로그인 쿠키 받아오기\n",
        "print(type(co))\n",
        "\n",
        "for i in co:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "4x3Uy9YYQefF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%config Completer.use_jedi = False #jupyter note 탭기능이 안될때 사용"
      ],
      "metadata": {
        "id": "io15w88dRW6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyperclip # ctrl + C, ctrl + V 기능을 가진 라이브러리"
      ],
      "metadata": {
        "id": "ek25yAsGR88q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import pyperclip\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "from datetime import datetime,timedelta\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "global titleUrl  # 셀레니움 동작에서 고정적으로 필요한 변수를 미리 선언\n",
        "\n",
        "titleUrl='https://www.bomtoon.com/api'\n",
        "\n",
        "url = titleUrl+'/balcony-api-v2/contents/tab/details'\n",
        "\n",
        "\n",
        "headers = {\n",
        "  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',\n",
        "  'X-Balcony-Id': 'BOMTOON_COM',\n",
        "  'Content-Type': 'application/json',\n",
        "  'Authorization': 'Bearer testw'}\n",
        "\n",
        "# 만화(=툰) 고유 id 들\n",
        "#data ={'contentsIds':'34157,33735,20381,14320,20617,34064,18147,16531,33753,34138,29926,22965,32089,21329,27572,28753,32598,30044,22699,21149,34077,28480,26487','contentsThumbnailType':'VERTICAL,MAIN,SQUARE'}\n",
        "\n",
        "def trans_datatime(unix_timestamp): # JSON에서 댓글을 보면 날짜 부분이 정수로 표현이 되어 있어서 정규화하는 함수\n",
        "    unix_timestamp_sec = unix_timestamp / 1000\n",
        "    dt_object = datetime.utcfromtimestamp(unix_timestamp_sec)\n",
        "    korea_dt_object = dt_object + timedelta(hours=9)\n",
        "    formatted_date = korea_dt_object.strftime('%Y-%m-%d %H:%M:%S')\n",
        "    return formatted_date\n",
        "\n",
        "def AweekUrl(headers_):   #주간 툰의 고유 id 전체 data 추출 함수\n",
        "    aweekAll_list=[]\n",
        "    aweek=['monday','tuesday','wednesday','thursday','friday','saturday','sunday','ten','COMPLETE']\n",
        "    for i in aweek:\n",
        "        valWeek=titleUrl+'/balcony-api-v2/contents/tab/schedule/comic?groupMenu={}&isIncludeTen=false&sort=POPULAR&adultToggle=false'.format(i.upper())\n",
        "        res = requests.request(\"GET\", valWeek, headers=headers_)\n",
        "        #print(valWeek)\n",
        "        aweekJson = res.json()\n",
        "        #print(aweekJson['data'])\n",
        "        aweekAll_list.append(aweekJson['data'])\n",
        "    return aweekAll_list\n",
        "\n",
        "\n",
        "def GetBomJakas(url_,headers_,getData): # 작가당 웹툰 작가 이름 및 ID 2차원 리스트\n",
        "\n",
        "    jakaList_name=[[]]\n",
        "\n",
        "    res=requests.post(url_,  headers=headers_, json=getData)\n",
        "    dic_res=res.json()\n",
        "    for i in range(len(dic_res['data'])):\n",
        "        #print(dic_res['data'][i]['alias'])\n",
        "        jakaList_name.append([dic_res['data'][i]['alias'],dic_res['data'][i]['creators']])\n",
        "\n",
        "    return jakaList_name # 작가당 웹툰 작가 이름 및 ID 2차원 리스트\n",
        "\n",
        "\n",
        "def getCustData(createData):  # data 데이터 재생성 반화\n",
        "    val=','.join(map(str,createData))\n",
        "    getDic={'contentsIds' : val ,'contentsThumbnailType':'VERTICAL,MAIN,SQUARE'}\n",
        "    return getDic\n",
        "\n",
        "\n",
        "def getPage_count(getID,headers_):  # 개별 작가의 댓글 총갯수\n",
        "    totElement=0\n",
        "    try:\n",
        "        url_=titleUrl+'/balcony-api/comment/contents/{}{}'.format(getID,'?isBest=true&page=0')\n",
        "        res = requests.request(\"GET\", url_, headers=headers_)\n",
        "        dic_res=res.json()\n",
        "        if dic_res['data'] == None:\n",
        "            print('no data')\n",
        "        else:\n",
        "            totElement=int(dic_res['data']['comment']['totalPages'])\n",
        "\n",
        "    except IndexError: ##인덱스 벗어나는 에러를 잡았을 경우에 대해서만!\n",
        "        print(\"페이지가 없습니다.\")\n",
        "\n",
        "    return totElement  # 개별 작가의 댓글 총갯수\n",
        "\n",
        "\n",
        "\n",
        "#작가명,작가ID,댓글총갯수,댓글총페이지 | 게시물인덱스,댓글쓴날짜,댓글의idx,작품명,작품회차,댓글자명,댓글,좋아요횟수\n",
        "c_write,c_writerID,c_replayCount,c_replayPage,c_commtIdx,c_day,c_nanoreIdx,c_title,c_cha,c_replayName,c_comment,c_like=[],[],[],[],[],[],[],[],[],[],[],[]\n",
        "\n",
        "selfcwd=os.getcwd()\n",
        "fold=selfcwd.split('\\\\')\n",
        "\n",
        "\n",
        "def GetAll_Comment(getID, getName, headers_, pageCount=0):\n",
        "    # print(titleUrl+'/balcony-api/comment/contents/{}'.format(getID[0]))\n",
        "    start = time.time()\n",
        "    totElement = 0\n",
        "    print('==>! ',getID, getName, pageCount)\n",
        "    try:\n",
        "        for su in range(int(pageCount)):\n",
        "            url_ = titleUrl + '/balcony-api/comment/contents/{}?isBest=true&page={}'.format(getID, su)\n",
        "            print('_COMMENT :==> ', url_)\n",
        "            res = requests.request(\"GET\", url_, headers=headers_)\n",
        "            time.sleep(2)\n",
        "            dic_res = res.json()\n",
        "            #print('res 갯수 : ',len(dic_res))\n",
        "\n",
        "            # 추가 모드로 파일 열기\n",
        "            with open('C:\\\\Users\\\\'+fold[2]+'\\\\Downloads\\\\commentJSON2.txt', 'a', encoding='utf-8') as f:\n",
        "                f.write(str(dic_res))\n",
        "\n",
        "\n",
        "            if dic_res['data'] == None:\n",
        "                    print('no data')\n",
        "                    c_comment.append('page no data')\n",
        "            else:\n",
        "                totElement = int(dic_res['data']['comment']['numberOfElements'])\n",
        "\n",
        "                #print(f'작가명 : {getName}__ {getID}')\n",
        "\n",
        "                loopdata = dic_res['data']['comment']['content']\n",
        "                jIDX = 0\n",
        "                try:\n",
        "                    for j in range(25):   # 0페이지 댓글 전체만 추출 추출\n",
        "                            # 여기중복\n",
        "                            #print(f'작가명 : {getName[0]}')\n",
        "                            c_write.append(getName)\n",
        "                            #print(f'작가명ID : {getID[0]}')\n",
        "                            c_writerID.append(getID)\n",
        "                            #print('댓글 총 수 : {}'.format(totElement))\n",
        "                            c_replayCount.append(totElement)\n",
        "                            #print('댓글 총 페이지 수 : {}'.format(pageCount))\n",
        "                            c_replayPage.append(pageCount)\n",
        "                            # 여기중복 끝\n",
        "\n",
        "                            ymd = trans_datatime(loopdata[j]['createdAt'])\n",
        "\n",
        "                            #print(f'게시물 IDX :: {jIDX}')\n",
        "                            c_commtIdx.append(jIDX)\n",
        "                            jIDX += 1\n",
        "                            # print(ymd)\n",
        "                            c_day.append(ymd)\n",
        "\n",
        "                           # print(loopdata[j]['commentId'], loopdata[j]['title'],\n",
        "                           #     loopdata[j]['subTitle'], loopdata[j]['nickname'])\n",
        "\n",
        "                            c_nanoreIdx.append(loopdata[j]['commentId'])\n",
        "                            c_title.append(loopdata[j]['title'])\n",
        "                            c_cha.append(loopdata[j]['subTitle'])\n",
        "                            c_replayName.append(loopdata[j]['nickname'])\n",
        "                        #print(loopdata[j]['comment'],loopdata[j]['likeCount'] )\n",
        "                            c_comment.append(loopdata[j]['comment'])\n",
        "                            #print(loopdata[j]['comment'])\n",
        "                            c_like.append(loopdata[j]['likeCount'])\n",
        "\n",
        "                except IndexError:\n",
        "                    print('page end')\n",
        "                    dic_res={}\n",
        "\n",
        "    except IndexError:  # 인덱스 벗어나는 에러를 잡았을 경우에 대해서만!\n",
        "        print(\"페이지가 없습니다.\")\n",
        "\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"{(end - start)*60/10:.5f} sec\")\n",
        "    return 0\n",
        "\n",
        "\n",
        "writerLists=AweekUrl(headers)  #주간 작가의 id 전체 data 추출 함수\n",
        "\n",
        "realdata=getCustData(writerLists[8]) # post 전송데이터 생성\n",
        "twoList=GetBomJakas(url,headers,realdata)  #  [ [getID],[getName]]\n",
        "\n",
        "getID=list(map(lambda x: x[0], twoList[1:]))\n",
        "getName=list(map(lambda x: x[1], twoList[1:]))\n",
        "print(len(getID) , len(getName))\n",
        "    #print('tqdm ==> ' ,getID[t] )\n",
        "for p in tqdm(range(len(getID))):\n",
        "    pageCount=getPage_count(getID[p],headers)\n",
        "    #print(f'pageChk ==> {pageCount} :: {getID[p]}')\n",
        "    GetAll_Comment(getID[p],getName[p],headers,pageCount)"
      ],
      "metadata": {
        "id": "XnijlJ1HSDzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pddata = {'작가명': pd.Series(c_write)\n",
        "          ,'작가ID': pd.Series(c_writerID)\n",
        "          ,'댓글총갯수':pd.Series(c_replayCount)\n",
        "          ,'댓글총페이지':pd.Series(c_replayPage)\n",
        "          ,'게시물인덱스':pd.Series(c_commtIdx)\n",
        "          ,'댓글쓴날짜':pd.Series(c_day)\n",
        "          ,'댓글의idx':pd.Series(c_nanoreIdx)\n",
        "          ,'작품명':pd.Series(c_title)\n",
        "          ,'작품회차':pd.Series(c_cha)\n",
        "          ,'댓글자명':pd.Series(c_replayName)\n",
        "          ,'댓글':pd.Series(c_comment)\n",
        "          ,'좋아요횟수':pd.Series(c_like)\n",
        "\n",
        "       }\n",
        "df = pd.DataFrame(pddata,columns=['작가명','작가ID','댓글총갯수','댓글총페이지','게시물인덱스','댓글쓴날짜','댓글의idx','작품명','작품회차','댓글자명','댓글','좋아요횟수'])\n",
        "\n",
        "#작가명,작가ID,댓글총갯수,댓글총페이지 | 게시물인덱스,댓글쓴날짜,댓글의idx,작품명,작품회차,댓글자명,댓글,좋아요횟수\n",
        "#del[[df]]\n",
        "\n",
        "#df\n",
        "selfcwd=os.getcwd()\n",
        "fold=selfcwd.split('\\\\')\n",
        "# 데이터프레임을 CSV 파일로 저장\n",
        "\n",
        "## replay titleUrl+'/balcony-api/comment/reply/CONTENTS/1654517?isBest=true&page=0\n",
        "if not os.path.exists('C:\\\\Users\\\\'+fold[2]+'\\\\Downloads\\\\output.csv'):\n",
        "    df.to_csv('C:\\\\Users\\\\'+fold[2]+'\\\\Downloads\\\\output.csv', index=False, mode='w', encoding='utf-8-sig')\n",
        "else:\n",
        "    df.to_csv('C:\\\\Users\\\\'+fold[2]+'\\\\Downloads\\\\output.csv', index=False, mode='a', encoding='utf-8-sig', header=False)\n"
      ],
      "metadata": {
        "id": "VlH5Ht56U4NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 크롤링 데이터 분석\n",
        "## 주요함수\n",
        "- pd.read_csv('경로.파일형식') : 데이터 읽기\n",
        "- df.info : 데이터 프레임 정보표시\n",
        "- df.isna().sum() : 결측츠 표시\n",
        "- df.dropna(변수 = ['열']) : 결측치 있는 특정 열의 행 제거\n",
        "- df.to_csv('파일명.파일형식', index = False) : 파일 저장, 인덱스 없음\n"
      ],
      "metadata": {
        "id": "Br5OkF9YVeuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df1 = pd.read_csv('/content/AllReview_Jo.csv')\n",
        "df1"
      ],
      "metadata": {
        "id": "F7VMRd2CVrNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.info() # 데이터 프레임 정보 표시"
      ],
      "metadata": {
        "id": "8M9I0eCPVsxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.isna().sum() # 결측치 표시"
      ],
      "metadata": {
        "id": "fC1V2mwnVuH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df1.dropna(subset = ['댓글']) # 결측치 있는 댓글 행 제거"
      ],
      "metadata": {
        "id": "M6pxkq2QVzz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.to_csv(\"AllReviewMVDrop_BUM.csv\", index = False)"
      ],
      "metadata": {
        "id": "qUfVlSAEV2Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 리뷰 코퍼스 작성\n",
        "## 주요함수\n",
        "- 변수.head() : 불러온 데이터 요약 확인, 기본 5줄\n",
        "- [review for review in review_data['댓글'] if type(review) is str] : for 문, review data의 타입이 str이면 불러들임\n",
        "- pd.DataFrame(key : value) : key, value 데이터 프레임화\n"
      ],
      "metadata": {
        "id": "IoZ9o-YpWteF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "review_data = pd.read_csv('/content/AllReviewMVDrop_BUM.csv') #결측치 제거된 데이터 업로드 후 읽음 \"봄툰csv 디렉터리의 dr\"\n",
        "review_data.head() #불러온 데이터 빠르게 확인\n",
        "\n",
        "print('데이터 개수: {}'.format(len(review_data)))\n",
        "\n",
        "# 문자열 아닌 데이터 모두 제거\n",
        "review_data = [review for review in review_data['댓글'] if type(review) is str]\n",
        "\n",
        "# 리스트를 DataFrame으로 변환\n",
        "review_df = pd.DataFrame({'댓글': review_data})\n",
        "\n",
        "# CSV 파일로 내보내기\n",
        "review_df.to_csv('AllReviewCorpus.csv', index=False)\n",
        "\n",
        "# XLSX 파일로 내보내기 (openpyxl 라이브러리가 필요할 수 있습니다)\n",
        "review_df.to_excel('AllReviewCorpus.xlsx', index=False)"
      ],
      "metadata": {
        "id": "WWLXDSwJW2_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 전처리\n",
        "## 주요함수\n",
        "- from konlpy.tag import Okt : Okt 형태소 라이브러리, 태그 포함\n",
        "- np.array(변수) : numpy\n",
        "- re.sub(r'[^ㄱ-ㅎ|ㅏ-ㅣ|가-힣|a-zA-Z0-9]+', '', review) : review 특수문자, 숫자, 한글 모음/자음 공란 변경하는 정규화 표현식\n",
        "- okt.nouns(변수) : 형태소 분석 중 명사 추출\n",
        "- ' '.join([noun for noun in nouns if noun not in stopwords]) : 복수의 공란 한 개로 변경"
      ],
      "metadata": {
        "id": "rzVxJu7WXjqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install konlpy 인스톨하세요\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# 형태소 분석기 초기화\n",
        "okt = Okt()\n",
        "\n",
        "# 데이터 불러오기\n",
        "data = pd.read_csv('/content/AllReviewCorpus_Moon.csv')\n",
        "print('데이터 개수:', len(data))\n",
        "\n",
        "# 문자열이 아닌 데이터 제거\n",
        "data_1 = data['댓글'].dropna().map(str)\n",
        "\n",
        "# 불용어 목록 파일을 불러옵니다.\n",
        "with open('/content/stopword_ko_Main.txt', 'r', encoding='utf-8') as f:\n",
        "    stopwords = np.array([line.strip().replace('\\r', '') for line in f.readlines()]) #속도 개선을 위한 np.array\n",
        "\n",
        "# 한국어만 남겨서 명사 추출 및 불용어 처리\n",
        "filtered_review = []\n",
        "for review in tqdm(data_1, desc=\"리뷰 처리 중\"):\n",
        "    review = re.sub(r'[^ㄱ-ㅎ|ㅏ-ㅣ|가-힣|a-zA-Z0-9]+', '', review)\n",
        "    nouns = okt.nouns(review)  # 형태소 분석을 통해 명사만 추출\n",
        "\n",
        "    # numpy 배열을 사용하여 불용어를 필터링합니다.\n",
        "    # filtered_nouns = ' '.join([noun for noun in nouns if noun not in stopwords])\n",
        "    # filtered_review.append(filtered_nouns)\n",
        "    filtered_review.append(nouns)\n"
      ],
      "metadata": {
        "id": "W1U0brSzXd8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 전처리\n",
        "## 주요함수\n",
        "- fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf' : 나눔바른고딕 import\n",
        "- WordCloud(font_path=폰트 변수, background_color='색', width=가로 크기, height=세로 크기, stopwords=set(불용어 변수, 중복제거)).generate(' '.join(filtered_review))\n",
        "- plt.figure(figsize=(가로, 세로))\n",
        "- plt.imshow(워드클라우드 변수, interpolation='표시 형식')\n",
        "- plt.axis('off') :\n",
        "- plt.show() : 워드클라우드 표시"
      ],
      "metadata": {
        "id": "3wPnZAnKYb_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#한글 글꼴 설치\n",
        "!apt -qq -y install fonts-nanum > /dev/null\n",
        "!pip install wordcloud\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 다시 numpy 배열로 변환합니다. (속도개선)\n",
        "filtered_review = np.array(filtered_review)\n",
        "\n",
        "# 워드 클라우드 생성 및 출력\n",
        "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "wordcloud = WordCloud(font_path=fontpath, background_color='white', width=800, height=800, stopwords=set(stopwords)).generate(' '.join(filtered_review))\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q9ISpdCUYbAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. PyLDA\n",
        "## 주요함수\n",
        "- TfidfVectorizer(max_features= 최대 열, max_df = 0.5, smooth_idf=True)\n",
        "- vectorizer.fit_transform(변수)\n",
        "- LatentDirichletAllocation(n_components=20, random_state=777)\n",
        "- lda_model.fit(X)\n",
        "- def get_topics(components, feature_names, n=5):\n",
        "     for idx, topic in enumerate(components):\n",
        "         print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])\n",
        "-pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.lda_model.prepare(\n",
        "                        lda_model = lda_model,\n",
        "                       dtm = X,\n",
        "                       vectorizer = vectorizer,\n",
        "                       mds='tsne')\n",
        "- pyLDAvis.display(변수)"
      ],
      "metadata": {
        "id": "mwZ5BYOFZI8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups"
      ],
      "metadata": {
        "id": "AOXabA56Zbjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features= 1000, # 상위 1,000개의 단어를 보존\n",
        "max_df = 0.5, smooth_idf=True)\n",
        "\n",
        "X = vectorizer.fit_transform(clean_review[0])\n",
        "\n",
        "# TF-IDF 행렬의 크기 확인\n",
        "print('TF-IDF 행렬의 크기 :',X.shape)"
      ],
      "metadata": {
        "id": "Qn7SGyFSZc0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "# 모델 객체 생성\n",
        "lda_model = LatentDirichletAllocation(n_components=20, random_state=777)"
      ],
      "metadata": {
        "id": "_acPfr3RZd8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit transfom\n",
        "lda_model.fit(X)"
      ],
      "metadata": {
        "id": "JkC3OkI2ZfVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 집합. 1,000개의 단어가 저장됨.\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "def get_topics(components, feature_names, n=5):\n",
        "    for idx, topic in enumerate(components):\n",
        "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])\n",
        "\n",
        "get_topics(lda_model.components_,terms)"
      ],
      "metadata": {
        "id": "Nv0JizCMZhe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시각화\n",
        "# LDAvis 는 토픽 모델링에 자주 이용되는 Latent Dirichlet Allocation (LDA) 모델의 학습 결과를 시각적으로 표현하는 라이브러리입니다\n",
        "!pip install -U pyLDAvis"
      ],
      "metadata": {
        "id": "-0_xF-16Zh7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LDA 시각화를 진행해봅시다.\n",
        "import pyLDAvis\n",
        "import pyLDAvis.lda_model\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.lda_model.prepare(\n",
        "                        lda_model = lda_model,\n",
        "                       dtm = X,\n",
        "                       vectorizer = vectorizer,\n",
        "                       mds='tsne')\n",
        "pyLDAvis.display(vis)"
      ],
      "metadata": {
        "id": "euKZts0eZkN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA 정리"
      ],
      "metadata": {
        "id": "2XujBdQj3aPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features= 1000, # 상위 1,000개의 단어를 보존\n",
        "max_df = 0.5, smooth_idf=True)\n",
        "X = vectorizer.fit_transform(clean_review[0])\n",
        "print('TF-IDF 행렬의 크기 :',X.shape)\n",
        "\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "lda_model = LatentDirichletAllocation(n_components=20, random_state=777)\n",
        "lda_model.fit(X)\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "def get_topics(components, feature_names, n=5):\n",
        "    for idx, topic in enumerate(components):\n",
        "     print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])\n",
        "    get_topics(lda_model.components_,terms)\n",
        "\n",
        "!pip install -U pyLDAvis\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.lda_model\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.lda_model.prepare(\n",
        "lda_model = lda_model,\n",
        "dtm = X,\n",
        "vectorizer = vectorizer,\n",
        "mds='tsne')\n",
        "pyLDAvis.display(vis)\n",
        "\n",
        "\n",
        "# 1. **TF-IDF 행렬 생성**:\n",
        "#    - `TfidfVectorizer`를 사용하여 텍스트 데이터에서 TF-IDF 행렬을 생성합니다. 이 행렬은 텍스트 내 단어의 중요도를 수치화한 것입니다.\n",
        "#    - `max_features=1000`은 가장 중요한 상위 1,000개의 단어만을 사용하겠다는 의미입니다.\n",
        "#    - `max_df=0.5`는 문서의 50% 이상에서 등장하는 단어는 제외하겠다는 의미입니다. 너무 자주 등장하는 단어는 주제 분석에 도움이 되지 않을 수 있기 때문입니다.\n",
        "#    - `smooth_idf=True`는 IDF 계산 시 분모가 0이 되는 것을 방지하기 위해 로그 스무딩을 적용합니다.\n",
        "# 2. **LDA 모델 생성 및 적합**:\n",
        "#    - `LatentDirichletAllocation`을 사용하여 LDA(Latent Dirichlet Allocation) 모델을 생성합니다. LDA는 문서의 주제를 추출하는 데 사용되는 모델입니다.\n",
        "#    - `n_components=20`은 추출할 주제의 수를 20개로 설정합니다.\n",
        "#    - `random_state=777`은 결과의 재현성을 위해 난수 생성기의 시드를 설정합니다.\n",
        "#    - 생성된 LDA 모델을 앞서 생성한 TF-IDF 행렬에 적합시킵니다.\n",
        "# 3. **주제와 관련된 단어 출력**:\n",
        "#    - `get_topics` 함수는 각 주제에 대해 가장 중요한 단어와 그 중요도를 출력합니다.\n",
        "#    - `n=5`는 각 주제별로 상위 5개의 중요 단어를 출력하겠다는 의미입니다.\n",
        "# 4. **LDA 모델 시각화**:\n",
        "#    - `pyLDAvis` 라이브러리를 사용하여 LDA 모델의 결과를 시각적으로 표현합니다. 이를 통해 각 주제가 어떤 단어들로 구성되어 있는지, 주제들 간의 관계는 어떠한지 쉽게 파악할 수 있습니다.\n",
        "#    - `pyLDAvis.enable_notebook()`은 Jupyter Notebook 내에서 시각화를 활성화합니다.\n",
        "#    - `pyLDAvis.lda_model.prepare` 함수는 LDA 모델과 TF-IDF 행렬, 그리고 사용된 벡터라이저를 입력으로 받아 시각화에 필요한 데이터를 준비합니다.\n",
        "#    - `pyLDAvis.display(vis)`는 준비된 시각화 데이터를 실제로 화면에 표시합니다.\n",
        "# 이 코드를 통해 텍스트 데이터 내 주요 주제를 추출하고 시각화하여 분석할 수 있습니다."
      ],
      "metadata": {
        "id": "rOc0W54e3jdL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}