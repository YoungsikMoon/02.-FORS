{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN/08id6yka0hubrIzpIk0k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoungsikMoon/FORS/blob/main/%EC%A1%B0%EC%98%81%EC%88%98/FORS_%EC%A1%B0%EC%98%81%EC%88%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Whisper AI fine-tuning í•˜ëŠ” ì½”ë“œ"
      ],
      "metadata": {
        "id": "Bk4cF7T2ApIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WhisperAI_fine_tune_version6"
      ],
      "metadata": {
        "id": "-zKatACsAq_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRA (Low Rank Adaptation) ì™€ PEFT (Paramater Efficient Fine Tuning)ë¥¼ ì ìš©í•œ Larger Whisper fine-tuning âš¡ï¸\n"
      ],
      "metadata": {
        "id": "YyhW-6uLAwmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* ì†Œë¹„ì GPUì˜ VRAMì´ 8GB ë¯¸ë§Œì¸ í™˜ê²½ì—ì„œë„ full-finetuningê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ì œê³µí•¨\n",
        "*  ğŸ¤— Transformers and PEFT ëª¨ë¸ê³¼ Common Voice 13.0 datasetë¥¼ ì‚¬ìš©í•˜ì—¬ Whisper fine-tuneí•˜ëŠ” ê³¼ì • ì„¤ëª…í•¨\n",
        "* PEFTì™€ bitsandbytesë¥¼ í™œìš©í•˜ì—¬ ë¬´ë£Œ T4 GPU(16GB VRAM)ë¥¼ ì‚¬ìš©í•˜ì—¬ whisper-large-v2 ì²´í¬í¬ì¸íŠ¸ë¥¼ ì›í™œí•˜ê²Œ í•™ìŠµ"
      ],
      "metadata": {
        "id": "GI9CES8OCw5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì™œ Parameter Efficient Fine Tuning [PEFT](https://github.com/huggingface/peft)ë¥¼ ì‚¬ìš©í•´ì•¼ ë˜ëŠ”ê°€?\n"
      ],
      "metadata": {
        "id": "AwOEdVeIA0gR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ëª¨ë¸ ì‚¬ì´ì¦ˆ ì¦ê°€ë¡œ fine tuningí•˜ëŠ” ê²ƒì´ ê³„ì‚° ë³µì¡ì„± ì¦ê°€ì™€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€\n",
        "    * ì˜ˆë¥¼ ë“¤ì–´, Whisper-large-v2 ëª¨ë¸ì„ ì™„ì „í•œ ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•´ ì•½ 24GBì˜ GPU VRAMì´ í•„ìš”í•˜ë©°, ê° ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì€ ì•½ 7GBì˜ ì €ì¥ ê³µê°„ì„ í•„ìš”í•¨\n",
        "\n",
        "    * ì œí•œì ì¸ í™˜ê²½ì—ì„œ bottleneck ë°œìƒí•˜ê³  ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì–»ê¸° í˜ë“¦\n",
        "* PEFT\n",
        "    * íš¨ê³¼ì ìœ¼ë¡œ parameterë¥¼ ì¤„ì—¬ì„œ fine tuning ì†ë„ ê°œì„ \n",
        "    * ëª©ì : ë³‘ëª© í˜„ìƒì„ í•´ê²°\n",
        "    * ì ‘ê·¼ë²•(ì˜ˆ: ì €ìˆ˜ì¤€ ì ì‘): ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì˜ ëŒ€ë¶€ë¶„ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ë™ê²°ì‹œí‚¤ë©´ì„œ ì¶”ê°€ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ì˜ ì¼ë¶€ë§Œ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ê³„ì‚° ë° ì €ì¥ ë¹„ìš© í¬ê²Œ ì¤„ì„\n",
        "        * ëŒ€ê·œëª¨ ëª¨ë¸ì˜ ì „ì²´ ë¯¸ì„¸ ì¡°ì • ì¤‘ ê´€ì°°ë˜ëŠ” catastrophic forgetting ë¬¸ì œë¥¼ ê·¹ë³µí•  ìˆ˜ ìˆìŒ\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1-qDk6vUA2DX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LoRAê°€ ë¬´ì—‡ì¸ê°€?\n"
      ],
      "metadata": {
        "id": "hjtgo0vBA7tL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* PEFTì—ì„œ ì—¬ëŸ¬ ë§¤ê°œë³€ìˆ˜ íš¨ìœ¨ì ì¸ ê¸°ìˆ ì„ ê¸°ë³¸ìœ¼ë¡œ ì œê³µ\n",
        "    * ê·¸ ì¤‘ í•˜ë‚˜ì¸ Low Rank Adaptation (LoRA)\n",
        "        * ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë™ê²°í•˜ê³  Transformer ì•„í‚¤í…ì²˜ì˜ ê° ë ˆì´ì–´ì— í›ˆë ¨ ê°€ëŠ¥í•œ ë­í¬ ë¶„í•´ í–‰ë ¬ì„ ì‚½ì… (High Rank ì¦‰ ë§ì€ ì—°ê²°ì´ ë˜ì–´ ìˆëŠ” ê²ƒë“¤ë³´ë‹¤ ì—°ê²°ì´ ì ì€ Low Rankë¡œ ë§Œë“¤ì–´ì„œ ê³„ì‚°ëŸ‰ì„ ì¤„ì„)\n",
        "            * Downstream ì‘ì—…ì— ëŒ€í•œ í›ˆë ¨ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ ìˆ˜ê°€ í¬ê²Œ ê°ì†Œ\n"
      ],
      "metadata": {
        "id": "_IqBnkBlC0em"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### í†µê³„ë¡œ ë³´ëŠ” PEFT íš¨ê³¼\n"
      ],
      "metadata": {
        "id": "20Ii_PmCBEZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Full fine-tuning of Whisper-large-v2 checkpoint Vs. PEFT ì ìš© ëª¨ë¸\n",
        "\n",
        "    1. GPU VRAMì´ 8GB ë¯¸ë§Œì¸ í™˜ê²½ì—ì„œ 16ì–µ ê°œì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì • ğŸ¤¯\n",
        "    2. í›¨ì”¬ ì ì€ ìˆ˜ì˜ í›ˆë ¨ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê±°ì˜ 5ë°° ë” í° ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš© ê°€ëŠ¥ ğŸ“ˆ\n",
        "    3. ìƒì„±ëœ ì²´í¬í¬ì¸íŠ¸ëŠ” ì›ë³¸ ëª¨ë¸ì˜ í¬ê¸°ì˜ 1%ì¸ ì•½ 60MB ğŸš€\n",
        "* ê¸°ì¡´ ğŸ¤— transformers Whisperì—ì„œ ë³€í˜•ì´ ë§ì´ ë˜ì§€ ì•Šì•˜ìŒ"
      ],
      "metadata": {
        "id": "Wg0LhH75C6HU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### í™˜ê²½ì„¤ì •\n"
      ],
      "metadata": {
        "id": "MiCJiRtgBHYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* Python package->Whisper ëª¨ë¸ fine tuningí•˜ê¸° ìœ„í•´ ì‚¬ìš©\n",
        "  * `datasets`:í•™ìŠµ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì¤€ë¹„\n",
        "  * `transformers`: Whisper ëª¨ë¸ì„ ë¡œë“œí•˜ê³  í›ˆë ¨\n",
        "  * `librosa`: ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì „ì²˜ë¦¬\n",
        "  * `evaluate` &  `jiwer`:ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€\n",
        "  * `PEFT`, `bitsandbytes`, `accelerate`: LoRAë¡œ ëª¨ë¸ê³¼ fine-tuning"
      ],
      "metadata": {
        "id": "Nqb9770SC8Gz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sACxsKfCAdZo"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets librosa evaluate jiwer gradio bitsandbytes==0.37 accelerate\n",
        "!pip install -q git+https://github.com/huggingface/peft.git@main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install typer==0.9.1"
      ],
      "metadata": {
        "id": "Rlp5jkcKBKHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* GPU í™•ë³´\n",
        "    * Google Colab Pro: V100 ë˜ëŠ” P100 GPUê°€ í• ë‹¹\n",
        "* GPU í™•ë³´ ë°©ë²•\n",
        "    * ëŸ°íƒ€ì„ -> ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½\n",
        "    * None -> GPU ë³€ê²½\n",
        "* GPU í• ë‹¹ í™•ì¸"
      ],
      "metadata": {
        "id": "fhZiYqOvBLxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "78HJfwwgBNTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colabì—ì„œ ì œê³µí•˜ëŠ” GPUì‚¬ìš©"
      ],
      "metadata": {
        "id": "kGYOX3puBO5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "N8VwGP0WBQPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We strongly advise you to upload model checkpoints directly the [Hugging Face Hub](https://huggingface.co/)\n",
        "whilst training. The Hub provides:\n",
        "- Integrated version control: you can be sure that no model checkpoint is lost during training.\n",
        "- Tensorboard logs: track important metrics over the course of training.\n",
        "- Model cards: document what a model does and its intended use cases.\n",
        "- Community: an easy way to share and collaborate with the community!\n",
        "\n",
        "\n",
        "* í•™ìŠµ ì¤‘ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì§ì ‘ Hugging Face Hubì— ì—…ë¡œë“œí•˜ëŠ” ê²ƒì„ ê°•ë ¥íˆ ê¶Œì¥\n",
        " * Hubë¥¼ ì‚¬ìš©í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê¸°ëŠ¥ì„ ì œê³µ:\n",
        "\n",
        " 1. í†µí•©ëœ ë²„ì „ ê´€ë¦¬: í›ˆë ¨ ì¤‘ì— ì–´ë– í•œ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë„ ì†ì‹¤ë˜ì§€ ì•ŠìŒ\n",
        " 2. Tensorboard ë¡œê·¸: í›ˆë ¨ ê³¼ì •ì—ì„œ ì¤‘ìš”í•œ ì§€í‘œë¥¼ ì¶”ì \n",
        " 3. ëª¨ë¸ ì¹´ë“œ: ëª¨ë¸ì´ ë¬´ì—‡ì„ í•˜ê³  ì˜ë„ëœ ì‚¬ìš© ì‚¬ë¡€ë¥¼ ë¬¸ì„œí™”\n",
        " 4. ì»¤ë®¤ë‹ˆí‹°: ì»¤ë®¤ë‹ˆí‹°ì™€ ì‰½ê²Œ ê³µìœ í•˜ê³  í˜‘ì—…\n",
        "\n",
        "* Hubì— ì—°ê²°\n",
        " * í”„ë¡¬í”„íŠ¸ì—ì„œ Hub ì¸ì¦ í† í°ì„ ì…ë ¥:"
      ],
      "metadata": {
        "id": "97s-5e1_BRa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "7PXhqHnPBUAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Whisper ëª¨ë¸ checkpointì™€ task ì„¤ì •"
      ],
      "metadata": {
        "id": "-IxwKzYUBVTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"openai/whisper-large-v2\"\n",
        "task = \"transcribe\""
      ],
      "metadata": {
        "id": "ZF_Yzhh8BYYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë°ì´í„°ì…‹ ë””í…Œì¼ì„ ì„¤ì • (ì–¸ì–´)"
      ],
      "metadata": {
        "id": "mPaxRfLnBZ0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"mozilla-foundation/common_voice_13_0\"\n",
        "language = \"Korean\"\n",
        "language_abbr = \"ko\" # Short hand code for the language we want to fine-tune"
      ],
      "metadata": {
        "id": "SKFpfEuZBb4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ë°ì´í„°ì…‹ ì˜¬ë¦¬ê¸°\n"
      ],
      "metadata": {
        "id": "6VEIzD2tBeW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Huggingface Dataset ì‚¬ìš©\n",
        " * ì ì€ ì½”ë“œë¡œ Common Voiceì˜ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì¤€ë¹„\n",
        "\n",
        "* í™•ì¸ ì ˆì°¨\n",
        " 1. Hugging Face Hubì˜ ì´ìš© ì•½ê´€ì„ ìˆ˜ë½í–ˆëŠ”ì§€ í™•ì¸:[mozilla-foundation/common_voice_13_0](https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0)\n",
        " 2. ë°ì´í„°ì…‹ì— ì•¡ì„¸ìŠ¤í•˜ê³  ë¡œì»¬ë¡œ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œ\n",
        "\n",
        "* í•™ìŠµ+ê²€ì¦ ë°ì´í„°ì…‹/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¶„ë¦¬"
      ],
      "metadata": {
        "id": "GkDJv-QgDARH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice[\"train\"] = load_dataset(dataset_name, language_abbr, split=\"train+validation\", use_auth_token=True)\n",
        "common_voice[\"test\"] = load_dataset(dataset_name, language_abbr, split=\"test\", use_auth_token=True)\n",
        "\n",
        "print(common_voice)"
      ],
      "metadata": {
        "id": "nbYnyTYtBgL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ì¼ë°˜ì ì¸ ASR(ìŒì„± ì¸ì‹) ë°ì´í„°ì…‹\n",
        "    * ì…ë ¥ ì˜¤ë””ì˜¤ ìƒ˜í”Œ(ì˜¤ë””ì˜¤)ê³¼ í•´ë‹¹ë˜ëŠ” í…ìŠ¤íŠ¸(ë¬¸ì¥)ë§Œ ì œê³µ\n",
        "* Common Voice\n",
        "    * ASRì—ëŠ” í•„ìš”í•˜ì§€ ì•Šì€ ì•…ì„¼íŠ¸ì™€ ë¡œì¼€ì¼ê³¼ ê°™ì€ ì¶”ê°€ ë©”íƒ€ë°ì´í„° ì •ë³´ê°€ í¬í•¨\n",
        "    * ì¼ë°˜ì ì¸ ìš©ë„ë¡œ ì‚¬ìš©í•˜ê³  ë¯¸ì„¸ ì¡°ì •ì„ ê³ ë ¤í•˜ê¸° ìœ„í•´ ë©”íƒ€ë°ì´í„° ì •ë³´ ë¬´ì‹œ"
      ],
      "metadata": {
        "id": "S3EHxQ4UBhiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = common_voice.remove_columns(\n",
        "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\", \"variant\"]\n",
        ")\n",
        "\n",
        "print(common_voice)"
      ],
      "metadata": {
        "id": "_nohoiLhBldK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### íŠ¹ì„± ì¶”ì¶œê¸°(Feature Extractor), í† í¬ë‚˜ì´ì €(Tokenizer), ê·¸ë¦¬ê³  ë°ì´í„°ì¤€ë¹„\n"
      ],
      "metadata": {
        "id": "Q0KdlbRhBlCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ASR íŒŒì´í”„ë¼ì¸ ì„¸ ë‹¨ê³„ë¡œ ë¶„í•´:\n",
        "\n",
        "1. Raw ì˜¤ë””ì˜¤ ì…ë ¥ì„ ì „ì²˜ë¦¬í•˜ëŠ” íŠ¹ì • ì¶”ì¶œê¸°\n",
        "2. ì‹œí€€ìŠ¤ ê°„ ë§¤í•‘ì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸\n",
        "3. ëª¨ë¸ ì¶œë ¥ì„ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ í›„ì²˜ë¦¬í•˜ëŠ” tokenizer\n",
        "\n",
        "\n",
        "* Whisper\n",
        "    * [WhisperFeatureExtractor](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperFeatureExtractor)ì™€ [WhisperTokenizer](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperTokenizer)ë¡œ êµ¬ì„±\n",
        "\n"
      ],
      "metadata": {
        "id": "tZaogWp9DCYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)"
      ],
      "metadata": {
        "id": "is9p-WzEBpXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperTokenizer\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)"
      ],
      "metadata": {
        "id": "j0e_p8K2BqMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* WhisperProcessor í´ë¼ìŠ¤\n",
        "    * íŠ¹ì„± ì¶”ì¶œê¸°ì™€ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë‘ ê°€ì§€ë¥¼ ëª¨ë‘ í•©ì¹©\n",
        "    * í•„ìš”ì— ë”°ë¼ ì˜¤ë””ì˜¤ ì…ë ¥ ë° ëª¨ë¸ ì˜ˆì¸¡ì— ì‚¬ìš© ê°€ëŠ¥\n",
        "\n",
        "* í•™ìŠµ ì¤‘ì— ë‘ ê°œì˜ ê°ì²´ë§Œ ì¶”ì  í•„ìš”: í”„ë¡œì„¸ì„œì™€ ëª¨ë¸"
      ],
      "metadata": {
        "id": "ARa3LohKBsZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
      ],
      "metadata": {
        "id": "4UJ-dMpABtce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ë°ì´í„° ì¤€ë¹„\n"
      ],
      "metadata": {
        "id": "Gpa0LrsKBvCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Common Voice ë°ì´í„°ì…‹ì˜ ì²« ë²ˆì§¸ ì˜ˆì œë¥¼ ì¶œë ¥í•˜ì—¬ ë°ì´í„°ì˜ í˜•ì‹ì„ ì‚´í´ë´„"
      ],
      "metadata": {
        "id": "kssVueqoDEaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(common_voice[\"train\"][0])"
      ],
      "metadata": {
        "id": "4C-Qaz03Bxl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Whisper ëª¨ë¸ ìƒ˜í”Œë§\n",
        "    * ì…ë ¥ ì˜¤ë””ì˜¤ëŠ” 48 kHz ìƒˆí”Œë§\n",
        "    * Whisper feature extractorì— ì „ë‹¬í•˜ê¸° ìœ„í•´ì„œ 16 kHzë¡œ ë‹¤ìš´ìƒ˜í”Œ ì§„í–‰\n",
        "* ìƒ˜í”Œë§ ì†ë„ ì„¤ì •\n",
        "    * Datasetì˜ [`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column) ë°©ë²• ì‚¬ìš©: ì˜¤ë””ì˜¤ ì…ë ¥ì„ ì˜¬ë°”ë¥¸ ìƒ˜í”Œë§ ì†ë„ë¡œ ì„¤ì •\n",
        "    * ì˜¤ë””ì˜¤ë¥¼ ë³€ê²½í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì˜¤ë””ì˜¤ ìƒ˜í”Œì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆë„ë¡ í•¨\n"
      ],
      "metadata": {
        "id": "H3ykq2UrBypT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Audio\n",
        "\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ],
      "metadata": {
        "id": "MmchcYGjB0C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Common Voice ë°ì´í„°ì…‹ì—ì„œ ì²« ë²ˆì§¸ ì˜¤ë””ì˜¤ ìƒ˜í”Œì„ ë‹¤ì‹œë¡œë“œí•˜ë©´ ì›í•˜ëŠ” ìƒ˜í”Œë§ ì†ë„ë¡œ ë‹¤ì‹œ ìƒ˜í”Œë§"
      ],
      "metadata": {
        "id": "_DrX-smNB3kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(common_voice[\"train\"][0])"
      ],
      "metadata": {
        "id": "rjqLt6R3B4VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* ëª¨ë¸ì— ë§ê²Œ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” í•¨ìˆ˜:\n",
        "\n",
        "1. batch[\"audio\"]ë¥¼ í˜¸ì¶œí•˜ì—¬ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ë‹¤ì‹œ ìƒ˜í”Œë§. ğŸ¤— DatasetsëŠ” í•„ìš”í•œ ëª¨ë“  ì¬ìƒ˜í”Œë§ ì‘ì—…ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜í–‰\n",
        "3. Feature extractorë¥¼ ì‚¬ìš©í•˜ì—¬ 1ì°¨ì› ì˜¤ë””ì˜¤ ë°°ì—´ì—ì„œ ë¡œê·¸ ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì…ë ¥ íŠ¹ì„±ì„ ê³„ì‚°\n",
        "3. Tokenizerë¥¼ ì‚¬ìš©í•˜ì—¬ transcriptsë¥¼ ë ˆì´ë¸” idsë¡œ ì¸ì½”ë”©\n"
      ],
      "metadata": {
        "id": "8WgVIxR_B5Sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids\n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch"
      ],
      "metadata": {
        "id": "NRcYfrHlB615"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜\n",
        "    * ë°ì´í„°ì…‹ì˜ .map ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  í•™ìŠµ ì˜ˆì œì— ì ìš© ê°€ëŠ¥\n",
        "    * num_proc: ëª‡ ê°œì˜ CPU ì½”ì–´ë¥¼ ì‚¬ìš©í•  ì§€ë¥¼ ì§€ì •,num_procë¥¼ 1ë³´ë‹¤ í¬ê²Œ ì„¤ì •í•˜ë©´ ë‹¤ì¤‘ ì²˜ë¦¬ê°€ í™œì„±í™” (ë‹¤ì¤‘ ì²˜ë¦¬ë¡œ .map ë©”ì„œë“œê°€ ì¤‘ë‹¨ë˜ëŠ” ê²½ìš° num_proc=1ë¡œ ì„¤ì •í•˜ê³  ë°ì´í„°ì…‹ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬)\n",
        "\n",
        "* Datasetì˜ ì‚¬ì´ì¦ˆì— ë”°ë¼ì„œ 20~30 ë¶„ ì •ë„ ê±¸ë¦¼\n"
      ],
      "metadata": {
        "id": "HFeLximJB8My"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2)"
      ],
      "metadata": {
        "id": "0WRDWCmhB-S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice[\"train\"]"
      ],
      "metadata": {
        "id": "5P3MOMOJB_HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### í•™ìŠµ ë° ê²€ì¦\n",
        "\n"
      ],
      "metadata": {
        "id": "cTzGBVYvCAaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* í›ˆë ¨ íŒŒì´í”„ë¼ì¸\n",
        "* [ğŸ¤— Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer)ê°€ ëŒ€ë¶€ë¶„ì˜ ì‘ì—…ì„ ì²˜ë¦¬:\n",
        "\n",
        "\n",
        "1. ë°ì´í„° collator ì •ì˜: ë°ì´í„° ì½œë ˆì´í„°ëŠ” ìš°ë¦¬ê°€ ì „ì²˜ë¦¬í•œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ëª¨ë¸ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” PyTorch í…ì„œë¡œ ì¤€ë¹„\n",
        "2. í‰ê°€ ì§€í‘œ: í‰ê°€ ì¤‘ì—ëŠ” ëª¨ë¸ì„ ê¸€ì ì˜¤ë¥˜ìœ¨  [word error rate (CER)](https://huggingface.co/metrics/cer)ì§€í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€\n",
        "3. ì‚¬ì „ í›ˆë ¨ëœ ì²´í¬í¬ì¸íŠ¸ load: ì‚¬ì „ í›ˆë ¨ëœ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•˜ê³  í›ˆë ¨ì„ ìœ„í•´ ì˜¬ë°”ë¥´ê²Œ êµ¬ì„±\n",
        "4. í›ˆë ¨ êµ¬ì„± ì •ì˜: ğŸ¤— Trainerê°€ í›ˆë ¨ ìŠ¤ì¼€ì¤„ì„ ì •ì˜ì— ì‚¬ìš©\n",
        "\n",
        "* ë¯¸ì„¸ ì¡°ì •í•œ í›„ì—ëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ëª¨ë¸ì„ í‰ê°€í•˜ì—¬ í•œêµ­ì–´ ìŒì„±ì„ ì˜¬ë°”ë¥´ê²Œ transcribe"
      ],
      "metadata": {
        "id": "WE2_imGHDHM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Collator ì •ì˜\n"
      ],
      "metadata": {
        "id": "1HoMXTkCCEU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ ìŒì„± ëª¨ë¸ì˜ ë°ì´í„° ì½œë ˆì´í„°\n",
        "    * Input_featuresì™€ labelsë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ ì°¨ë³„í™”\n",
        "    \n",
        "    - Input_features: feature extractorì— ì˜í•´ ì²˜ë¦¬\n",
        "    - labels: tokenizerì— ì˜í•´ ì²˜ë¦¬\n",
        "\n",
        "* Input_featuresëŠ” ì´ë¯¸ 30ì´ˆë¡œ íŒ¨ë”©ë˜ì–´ ìˆê³  íŠ¹ì„± ì¶”ì¶œê¸°ì— ì˜í•´ ê³ ì •ëœ ì°¨ì›ì˜ ë¡œê·¸ ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ìœ¼ë¡œ ë³€í™˜. ë”°ë¼ì„œ ìš°ë¦¬ê°€ í•´ì•¼ í•  ì¼ì€ input_featuresë¥¼ ë°°ì¹˜ ì²˜ë¦¬ëœ PyTorch í…ì„œë¡œ ë³€í™˜\n",
        "\n",
        "* labelsëŠ” íŒ¨ë”©ë˜ì§€ ì•ŠìŒ ë¨¼ì € ë°°ì¹˜ ë‚´ì—ì„œ ìµœëŒ€ ê¸¸ì´ì— ë§ê²Œ ì‹œí€€ìŠ¤ë¥¼ íŒ¨ë”©í•˜ê³ , tokenizerì˜ .pad ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œí€€ìŠ¤ë¥¼ íŒ¨ë”© íŒ¨ë”© í† í°ì€ ì†ì‹¤ì„ ê³„ì‚°í•  ë•Œ ê³ ë ¤ë˜ì§€ ì•Šë„ë¡ -100ìœ¼ë¡œ ëŒ€ì²´. ê·¸ëŸ° ë‹¤ìŒ ë ˆì´ë¸” ì‹œí€€ìŠ¤ì˜ ì‹œì‘ì—ì„œ BOS í† í°ì„ ì˜ë¼ì„œ í›ˆë ¨ ì¤‘ì— ë‚˜ì¤‘ì— ì´ë¥¼ ì¶”ê°€\n",
        "\n",
        "* ì´ì „ì— ì •ì˜í•œ WhisperProcessorë¥¼ í™œìš©í•˜ì—¬ íŠ¹ì„± ì¶”ì¶œê¸° ë° í† í¬ë‚˜ì´ì € ì‘ì—…ì„ ëª¨ë‘ ìˆ˜í–‰ ê°€ëŠ¥"
      ],
      "metadata": {
        "id": "JSYUN_-CDJul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "wX8T4B4qCGJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data collator ì´ˆê¸°í™” ì§„í–‰"
      ],
      "metadata": {
        "id": "WRDNGiVxCICZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ],
      "metadata": {
        "id": "EpyF337cCIvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### í‰ê°€ ì§€í‘œ"
      ],
      "metadata": {
        "id": "Izc2ZJQhCJ-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* ASR ì‹œìŠ¤í…œì„ í‰ê°€í•˜ê¸° ìœ„í•œ 'ì‚¬ì‹¤ìƒì˜' ì§€í‘œì¸ í•œ ë‹¨ì–´ ì˜¤ë¥˜ìœ¨(CER) ë©”íŠ¸ë¦­ì„ ì‚¬ìš©\n",
        "* ë” ë§ì€ ì •ë³´ëŠ” [ë¬¸ì„œ](https://huggingface.co/metrics/cer)ë¥¼ ì°¸ì¡°. ìš°ë¦¬ëŠ” ğŸ¤— Evaluateì—ì„œ CER ë©”íŠ¸ë¦­ì„ ë¡œë“œ"
      ],
      "metadata": {
        "id": "PPyFwm9eCLvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"cer\")"
      ],
      "metadata": {
        "id": "q_5KaaC8CMvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Â Pre-trained ëª¨ë¸ ë¡œë“œ"
      ],
      "metadata": {
        "id": "zehRf78NCOkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ì‚¬ì „ í›ˆë ¨ëœ Whisper ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œ\n",
        "    * ì´ ì‘ì—…ì€ ğŸ¤— Transformersë¥¼ ì‚¬ìš©í•˜ì—¬ ë§¤ìš° ê°„ë‹¨\n",
        "\n",
        "\n",
        "\n",
        "* ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ ëª¨ë¸ì„ 8ë¹„íŠ¸ë¡œ         \n",
        "    * ëª¨ë¸ì„ 1/4 ì •ë°€ë„(32ë¹„íŠ¸ì™€ ë¹„êµí–ˆì„ ë•Œ)ë¡œ ì–‘ìí™”í•˜ì—¬ ì„±ëŠ¥ ì†ì‹¤ì„ ìµœì†Œí™” [here](https://huggingface.co/blog/hf-bitsandbytes-integration)"
      ],
      "metadata": {
        "id": "8qJm2PvhCQiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, load_in_8bit=True, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "ivkkK8BMCRsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ëª¨ë¸ì˜ í›„ì²˜ë¦¬\n"
      ],
      "metadata": {
        "id": "K5-OWiwPCSyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. í›ˆë ¨ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ê¸° ìœ„í•´ 8ë¹„íŠ¸ ëª¨ë¸ì— ëª‡ ê°€ì§€ í›„ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ì ìš©\n",
        "2. ëª¨ë¸ ë ˆì´ì–´ë¥¼ ë™ê²°, í›ˆë ¨ê³¼ ëª¨ë¸ì˜ ì•ˆì •ì„±ì„ ìœ„í•´ ë ˆì´ì–´ ì •ê·œí™”ì™€ ì¶œë ¥ ë ˆì´ì–´ë¥¼ float32ë¡œ ìºìŠ¤íŒ…\n",
        "\n",
        "(ëª¨ë¸ ì•ˆì •ì„±ê³¼ layer normalization ë¶„ì„, float32ë¡œ ìºìŠ¤íŒ… í•˜ëŠ” ì´ìœ )"
      ],
      "metadata": {
        "id": "pvH50cFLDMlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "id": "CiWVYCsPCULa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model = prepare_model_for_kbit_training(model, output_embedding_layer_name=\"proj_out\")"
      ],
      "metadata": {
        "id": "I_DmsXAECU0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Whisper ëª¨ë¸ì€ ì¸ì½”ë”ì— ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì²´í¬í¬ì¸íŒ…ì€ grad ì—°ì‚°ì„ ë¹„í™œì„±. ì´ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì…ë ¥ì„ íŠ¹ë³„íˆ trainableí•˜ê²Œ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.\n"
      ],
      "metadata": {
        "id": "JENjOj3fCWNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_inputs_require_grad(module, input, output):\n",
        "    output.requires_grad_(True)\n",
        "\n",
        "model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)"
      ],
      "metadata": {
        "id": "HfyVl7UqCXYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Low-rank adapters (LoRA)ë¥¼ ëª¨ë¸ì— ì ìš©\n"
      ],
      "metadata": {
        "id": "vPPjkWEICYgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* peftì—ì„œ get_peft_model ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ PeftModelì„ ë¡œë“œí•˜ê³  ì €í¬ê°€ ì €ì°¨ì› ì–´ëŒ‘í„°(LoRA)ë¥¼ ì‚¬ìš©í•  ê²ƒì„ì„ ì§€ì •\n"
      ],
      "metadata": {
        "id": "TZof0_YTDOQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "-RHY3bHQCaNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1%**ì˜ í•™ìŠµ parameterë¥¼ ì‚¬ìš©í•˜ì˜€ê³  **Parameter-Efficient Fine-Tuning**ë¥¼ ì ìš©\n"
      ],
      "metadata": {
        "id": "ochIz600Cbd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### í›ˆë ¨ êµ¬ì„± ì •ì˜"
      ],
      "metadata": {
        "id": "3xZgpcjoCcv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ë§ˆì§€ë§‰ ë‹¨ê³„ì—ì„œëŠ” í›ˆë ¨ê³¼ ê´€ë ¨ëœ ëª¨ë“  ë§¤ê°œë³€ìˆ˜ë¥¼ ì •ì˜ í›ˆë ¨ ì¸ìì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ í•´ë‹¹ ë¬¸ì„œë¥¼ ì°¸ì¡° Seq2SeqTrainingArguments [docs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments)\n"
      ],
      "metadata": {
        "id": "qBStvREQCd4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"reach-vb/test\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-3,\n",
        "    warmup_steps=50,\n",
        "    num_train_epochs=1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    fp16=True,\n",
        "    per_device_eval_batch_size=8,\n",
        "    generation_max_length=128,\n",
        "    logging_steps=100,\n",
        "    max_steps=100, # only for testing purposes, remove this from your final run :)\n",
        "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
        "    label_names=[\"labels\"],  # same reason as above\n",
        ")"
      ],
      "metadata": {
        "id": "8bAfxwiKCeo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* PEFTë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒì—ëŠ” ëª‡ ê°€ì§€ ì£¼ì˜ê°€ í•„ìš”\n",
        "\n",
        "1. PeftModelì˜ forwardê°€ ê¸°ë³¸ ëª¨ë¸ì˜ forwardì˜ ì‹œê·¸ë‹ˆì²˜ë¥¼ ìƒì†í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— remove_unused_columns=False ë° label_names=[\"labels\"]ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •\n",
        "2. INT8 í›ˆë ¨ì—ëŠ” ìë™ ìºìŠ¤íŒ…ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— Trainerì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ ì œê³µë˜ëŠ” predict_with_generate í˜¸ì¶œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìë™ ìºìŠ¤íŒ…ì´ ìë™ìœ¼ë¡œ ì ìš©ë˜ì§€ ì•ŠìŒ\n",
        "3. ìë™ ìºìŠ¤íŒ…ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ Seq2SeqTrainerì— compute_metricsë¥¼ ì „ë‹¬í•  ìˆ˜ ì—†ìŒ. ë”°ë¼ì„œ Trainerë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ëŠ” ë™ì•ˆ compute_metricsë¥¼ ì£¼ì„ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n"
      ],
      "metadata": {
        "id": "zTQwzrWpCfzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
        "\n",
        "# This callback helps to save only the adapter weights and remove the base model weights.\n",
        "class SavePeftModelCallback(TrainerCallback):\n",
        "    def on_save(\n",
        "        self,\n",
        "        args: TrainingArguments,\n",
        "        state: TrainerState,\n",
        "        control: TrainerControl,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
        "\n",
        "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
        "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
        "\n",
        "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
        "        if os.path.exists(pytorch_model_path):\n",
        "            os.remove(pytorch_model_path)\n",
        "        return control\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=common_voice[\"train\"],\n",
        "    eval_dataset=common_voice[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    # compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    callbacks=[SavePeftModelCallback],\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ],
      "metadata": {
        "id": "UA4NRFR4ChER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "shqYZD0NCiFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our model is fine-tuned, we can push\n",
        "\n",
        "*   í•­ëª© ì¶”ê°€\n",
        "*   í•­ëª© ì¶”ê°€\n",
        "\n",
        "the model on to Hugging Face Hub, this will later help us directly infer the model from the model repo."
      ],
      "metadata": {
        "id": "7qL5wO52CkMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation and Inference"
      ],
      "metadata": {
        "id": "Pi2eDMnqCofB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On to the fun part, we've successfully fine-tuned our model. Now let's put it to test and calculate the WER on the `test` set.\n",
        "\n",
        "As with training, we do have a few caveats to pay attention to:\n",
        "1. Since we cannot use `predict_with_generate` function, we will hand roll our own eval loop with `torch.cuda.amp.autocast()` you can check it out below.\n",
        "2. Since the base model is frozen, PEFT model sometimes fails to recognise the language while decoding. To fix that, we force the starting tokens to mention the language we are transcribing. This is done via `forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"Marathi\", task=\"transcribe\")` and passing that too the `model.generate` call.\n",
        "\n",
        "That's it, let's get transcribing! ğŸ”¥\n"
      ],
      "metadata": {
        "id": "ZwEFCWVKCqWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer\n",
        "\n",
        "peft_model_id = \"reach-vb/whisper-large-v2-hindi-100steps\" # Use the same model ID as before.\n",
        "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\n",
        "    peft_config.base_model_name_or_path, load_in_8bit=True, device_map=\"auto\"\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "model.config.use_cache = True"
      ],
      "metadata": {
        "id": "jXaUINtDCrcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
        "\n",
        "eval_dataloader = DataLoader(common_voice[\"test\"], batch_size=8, collate_fn=data_collator)\n",
        "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n",
        "normalizer = BasicTextNormalizer()\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "normalized_predictions = []\n",
        "normalized_references = []\n",
        "\n",
        "model.eval()\n",
        "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
        "    with torch.cuda.amp.autocast():\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = (\n",
        "                model.generate(\n",
        "                    input_features=batch[\"input_features\"].to(\"cuda\"),\n",
        "                    forced_decoder_ids=forced_decoder_ids,\n",
        "                    max_new_tokens=255,\n",
        "                )\n",
        "                .cpu()\n",
        "                .numpy()\n",
        "            )\n",
        "            labels = batch[\"labels\"].cpu().numpy()\n",
        "            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
        "            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "            predictions.extend(decoded_preds)\n",
        "            references.extend(decoded_labels)\n",
        "            normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n",
        "            normalized_references.extend([normalizer(label).strip() for label in decoded_labels])\n",
        "        del generated_tokens, labels, batch\n",
        "    gc.collect()\n",
        "wer = 100 * metric.compute(predictions=predictions, references=references)\n",
        "normalized_wer = 100 * metric.compute(predictions=normalized_predictions, references=normalized_references)\n",
        "eval_metrics = {\"eval/wer\": wer, \"eval/normalized_wer\": normalized_wer}\n",
        "\n",
        "print(f\"{wer=} and {normalized_wer=}\")\n",
        "print(eval_metrics)"
      ],
      "metadata": {
        "id": "BxYB1BcbCsIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fin!\n",
        "\n",
        "If you made it all the way till the end then pat yourself on the back. Looking back, we learned how to train *any* Whisper checkpoint faster, cheaper and with negligible loss in WER.\n",
        "\n",
        "With PEFT, you can also go beyond Speech recognition and apply the same set of techniques to other pretrained models as well. Come check it out here: https://github.com/huggingface/peft ğŸ¤—\n",
        "\n",
        "Don't forget to tweet your results and tag us! [@huggingface](https://twitter.com/huggingface) and [@reach_vb](https://twitter.com/reach_vb) â¤ï¸"
      ],
      "metadata": {
        "id": "shSu9FdvCtdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Whisper AI fine-tuningì˜ ê²½ëŸ‰í™” ë²„ì „"
      ],
      "metadata": {
        "id": "ZzIomsfJDeOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "fasterWhisper_w_PEFT_finetune (quantizationê³¼ parameter efficient fine-tuning ê¸°ë²•)"
      ],
      "metadata": {
        "id": "ixJANAkIEfOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRA (Low Rank Adaptation) ì™€ PEFT (Paramater Efficient Fine Tuning)ë¥¼ ì ìš©í•œ Larger Whisper fine-tuning âš¡ï¸\n"
      ],
      "metadata": {
        "id": "dPzNx-FvDk_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ì†Œë¹„ì GPUì˜ VRAMì´ 8GB ë¯¸ë§Œì¸ í™˜ê²½ì—ì„œë„ full-finetuningê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ì œê³µí•¨\n",
        "*  ğŸ¤— Transformers and PEFT ëª¨ë¸ê³¼ Common Voice 13.0 datasetë¥¼ ì‚¬ìš©í•˜ì—¬ Whisper fine-tuneí•˜ëŠ” ê³¼ì • ì„¤ëª…í•¨\n",
        "* PEFTì™€ bitsandbytesë¥¼ í™œìš©í•˜ì—¬ ë¬´ë£Œ T4 GPU(16GB VRAM)ë¥¼ ì‚¬ìš©í•˜ì—¬ whisper-large-v2 ì²´í¬í¬ì¸íŠ¸ë¥¼ ì›í™œí•˜ê²Œ í•™ìŠµ"
      ],
      "metadata": {
        "id": "V7499VwMDwUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì™œ Parameter Efficient Fine Tuning [PEFT](https://github.com/huggingface/peft)ë¥¼ ì‚¬ìš©í•´ì•¼ ë˜ëŠ”ê°€?\n"
      ],
      "metadata": {
        "id": "bKotHLVADxtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ëª¨ë¸ ì‚¬ì´ì¦ˆ ì¦ê°€ë¡œ fine tuningí•˜ëŠ” ê²ƒì´ ê³„ì‚° ë³µì¡ì„± ì¦ê°€ì™€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€\n",
        "    * ì˜ˆë¥¼ ë“¤ì–´, Whisper-large-v2 ëª¨ë¸ì„ ì™„ì „í•œ ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•´ ì•½ 24GBì˜ GPU VRAMì´ í•„ìš”í•˜ë©°, ê° ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì€ ì•½ 7GBì˜ ì €ì¥ ê³µê°„ì„ í•„ìš”í•¨\n",
        "\n",
        "    * ì œí•œì ì¸ í™˜ê²½ì—ì„œ bottleneck ë°œìƒí•˜ê³  ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì–»ê¸° í˜ë“¦\n",
        "* PEFT\n",
        "    * íš¨ê³¼ì ìœ¼ë¡œ parameterë¥¼ ì¤„ì—¬ì„œ fine tuning ì†ë„ ê°œì„ \n",
        "    * ëª©ì : ë³‘ëª© í˜„ìƒì„ í•´ê²°\n",
        "    * ì ‘ê·¼ë²•(ì˜ˆ: ì €ìˆ˜ì¤€ ì ì‘): ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì˜ ëŒ€ë¶€ë¶„ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ë™ê²°ì‹œí‚¤ë©´ì„œ ì¶”ê°€ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ì˜ ì¼ë¶€ë§Œ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ê³„ì‚° ë° ì €ì¥ ë¹„ìš© í¬ê²Œ ì¤„ì„\n",
        "        * ëŒ€ê·œëª¨ ëª¨ë¸ì˜ ì „ì²´ ë¯¸ì„¸ ì¡°ì • ì¤‘ ê´€ì°°ë˜ëŠ” catastrophic forgetting ë¬¸ì œë¥¼ ê·¹ë³µí•  ìˆ˜ ìˆìŒ\n"
      ],
      "metadata": {
        "id": "TX3Hr4eYDzzU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LoRAê°€ ë¬´ì—‡ì¸ê°€?"
      ],
      "metadata": {
        "id": "X6fmVz1gD1PY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* PEFTì—ì„œ ì—¬ëŸ¬ ë§¤ê°œë³€ìˆ˜ íš¨ìœ¨ì ì¸ ê¸°ìˆ ì„ ê¸°ë³¸ìœ¼ë¡œ ì œê³µ\n",
        "    * ê·¸ ì¤‘ í•˜ë‚˜ì¸ Low Rank Adaptation (LoRA)\n",
        "        * ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë™ê²°í•˜ê³  Transformer ì•„í‚¤í…ì²˜ì˜ ê° ë ˆì´ì–´ì— í›ˆë ¨ ê°€ëŠ¥í•œ ë­í¬ ë¶„í•´ í–‰ë ¬ì„ ì‚½ì… (High Rank ì¦‰ ë§ì€ ì—°ê²°ì´ ë˜ì–´ ìˆëŠ” ê²ƒë“¤ë³´ë‹¤ ì—°ê²°ì´ ì ì€ Low Rankë¡œ ë§Œë“¤ì–´ì„œ ê³„ì‚°ëŸ‰ì„ ì¤„ì„)\n",
        "            * Downstream ì‘ì—…ì— ëŒ€í•œ í›ˆë ¨ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ ìˆ˜ê°€ í¬ê²Œ ê°ì†Œ"
      ],
      "metadata": {
        "id": "EaIZiLwGD3Hi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### í†µê³„ë¡œ ë³´ëŠ” PEFT íš¨ê³¼"
      ],
      "metadata": {
        "id": "xTb93GTdD4UK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Full fine-tuning of Whisper-large-v2 checkpoint Vs. PEFT ì ìš© ëª¨ë¸\n",
        "\n",
        "    1. GPU VRAMì´ 8GB ë¯¸ë§Œì¸ í™˜ê²½ì—ì„œ 16ì–µ ê°œì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì • ğŸ¤¯\n",
        "    2. í›¨ì”¬ ì ì€ ìˆ˜ì˜ í›ˆë ¨ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê±°ì˜ 5ë°° ë” í° ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš© ê°€ëŠ¥ ğŸ“ˆ\n",
        "    3. ìƒì„±ëœ ì²´í¬í¬ì¸íŠ¸ëŠ” ì›ë³¸ ëª¨ë¸ì˜ í¬ê¸°ì˜ 1%ì¸ ì•½ 60MB ğŸš€\n",
        "* ê¸°ì¡´ ğŸ¤— transformers Whisperì—ì„œ ë³€í˜•ì´ ë§ì´ ë˜ì§€ ì•Šì•˜ìŒ\n",
        "\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "q4qXzrZwD6AJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### í™˜ê²½ì„¤ì •"
      ],
      "metadata": {
        "id": "weCbrKZXEwuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Python package->Whisper ëª¨ë¸ fine tuningí•˜ê¸° ìœ„í•´ ì‚¬ìš©\n",
        "  * `datasets`:í•™ìŠµ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì¤€ë¹„\n",
        "  * `transformers`: Whisper ëª¨ë¸ì„ ë¡œë“œí•˜ê³  í›ˆë ¨\n",
        "  * `librosa`: ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì „ì²˜ë¦¬\n",
        "  * `evaluate` &  `jiwer`:ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€\n",
        "  * `PEFT`, `bitsandbytes`, `accelerate`: LoRAë¡œ ëª¨ë¸ê³¼ fine-tuning"
      ],
      "metadata": {
        "id": "Pxt2i78zExte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets librosa evaluate jiwer gradio bitsandbytes==0.37 accelerate\n",
        "!pip install -q git+https://github.com/huggingface/peft.git@main"
      ],
      "metadata": {
        "id": "gIJNDn4AEy43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install typer==0.9.1"
      ],
      "metadata": {
        "id": "HNplKiMeE0Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* GPU í™•ë³´\n",
        "    * Google Colab Pro: V100 ë˜ëŠ” P100 GPUê°€ í• ë‹¹\n",
        "* GPU í™•ë³´ ë°©ë²•\n",
        "    * ëŸ°íƒ€ì„ -> ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½\n",
        "    * None -> GPU ë³€ê²½\n",
        "* GPU í• ë‹¹ í™•ì¸"
      ],
      "metadata": {
        "id": "GIJ4GZPRE1eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "YyQlJSD_E2ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colabì—ì„œ ì œê³µí•˜ëŠ” GPUì‚¬ìš©"
      ],
      "metadata": {
        "id": "Ap5Vjg_zE3rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "QF40Dh4uE4ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We strongly advise you to upload model checkpoints directly the [Hugging Face Hub](https://huggingface.co/)\n",
        "whilst training. The Hub provides:\n",
        "- Integrated version control: you can be sure that no model checkpoint is lost during training.\n",
        "- Tensorboard logs: track important metrics over the course of training.\n",
        "- Model cards: document what a model does and its intended use cases.\n",
        "- Community: an easy way to share and collaborate with the community!\n",
        "\n",
        "\n",
        "* í•™ìŠµ ì¤‘ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì§ì ‘ Hugging Face Hubì— ì—…ë¡œë“œí•˜ëŠ” ê²ƒì„ ê°•ë ¥íˆ ê¶Œì¥\n",
        " * Hubë¥¼ ì‚¬ìš©í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê¸°ëŠ¥ì„ ì œê³µ:\n",
        "\n",
        " 1. í†µí•©ëœ ë²„ì „ ê´€ë¦¬: í›ˆë ¨ ì¤‘ì— ì–´ë– í•œ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë„ ì†ì‹¤ë˜ì§€ ì•ŠìŒ\n",
        " 2. Tensorboard ë¡œê·¸: í›ˆë ¨ ê³¼ì •ì—ì„œ ì¤‘ìš”í•œ ì§€í‘œë¥¼ ì¶”ì \n",
        " 3. ëª¨ë¸ ì¹´ë“œ: ëª¨ë¸ì´ ë¬´ì—‡ì„ í•˜ê³  ì˜ë„ëœ ì‚¬ìš© ì‚¬ë¡€ë¥¼ ë¬¸ì„œí™”\n",
        " 4. ì»¤ë®¤ë‹ˆí‹°: ì»¤ë®¤ë‹ˆí‹°ì™€ ì‰½ê²Œ ê³µìœ í•˜ê³  í˜‘ì—…\n",
        "\n",
        "* Hubì— ì—°ê²°\n",
        " * í”„ë¡¬í”„íŠ¸ì—ì„œ Hub ì¸ì¦ í† í°ì„ ì…ë ¥:"
      ],
      "metadata": {
        "id": "tf3IIAeME5s3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "R9JK1YqnE7cX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Whisper ëª¨ë¸ checkpointì™€ task ì„¤ì •"
      ],
      "metadata": {
        "id": "6XJO6vi7E95o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"openai/whisper-large-v2\"\n",
        "task = \"transcribe\""
      ],
      "metadata": {
        "id": "l37TMk6qE_BL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "ë°ì´í„°ì…‹ ë””í…Œì¼ì„ ì„¤ì • (ì–¸ì–´)"
      ],
      "metadata": {
        "id": "L9dofpWCE__w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"mozilla-foundation/common_voice_13_0\"\n",
        "language = \"Korean\"\n",
        "language_abbr = \"ko\" # Short hand code for the language we want to fine-tune"
      ],
      "metadata": {
        "id": "Q0_VpqAqFAnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ë°ì´í„°ì…‹ ì˜¬ë¦¬ê¸°\n"
      ],
      "metadata": {
        "id": "klhOGEvnFCq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Huggingface Dataset ì‚¬ìš©\n",
        " * ì ì€ ì½”ë“œë¡œ Common Voiceì˜ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì¤€ë¹„\n",
        "\n",
        "* í™•ì¸ ì ˆì°¨\n",
        " 1. Hugging Face Hubì˜ ì´ìš© ì•½ê´€ì„ ìˆ˜ë½í–ˆëŠ”ì§€ í™•ì¸:[mozilla-foundation/common_voice_13_0](https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0)\n",
        " 2. ë°ì´í„°ì…‹ì— ì•¡ì„¸ìŠ¤í•˜ê³  ë¡œì»¬ë¡œ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œ\n",
        "\n",
        "* í•™ìŠµ+ê²€ì¦ ë°ì´í„°ì…‹/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¶„ë¦¬"
      ],
      "metadata": {
        "id": "rKL-c15cFF0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice[\"train\"] = load_dataset(dataset_name, language_abbr, split=\"train+validation\", use_auth_token=True)\n",
        "common_voice[\"test\"] = load_dataset(dataset_name, language_abbr, split=\"test\", use_auth_token=True)\n",
        "\n",
        "print(common_voice)"
      ],
      "metadata": {
        "id": "6ZozN_tfFMa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ì¼ë°˜ì ì¸ ASR(ìŒì„± ì¸ì‹) ë°ì´í„°ì…‹\n",
        "    * ì…ë ¥ ì˜¤ë””ì˜¤ ìƒ˜í”Œ(ì˜¤ë””ì˜¤)ê³¼ í•´ë‹¹ë˜ëŠ” í…ìŠ¤íŠ¸(ë¬¸ì¥)ë§Œ ì œê³µ\n",
        "* Common Voice\n",
        "    * ASRì—ëŠ” í•„ìš”í•˜ì§€ ì•Šì€ ì•…ì„¼íŠ¸ì™€ ë¡œì¼€ì¼ê³¼ ê°™ì€ ì¶”ê°€ ë©”íƒ€ë°ì´í„° ì •ë³´ê°€ í¬í•¨\n",
        "    * ì¼ë°˜ì ì¸ ìš©ë„ë¡œ ì‚¬ìš©í•˜ê³  ë¯¸ì„¸ ì¡°ì •ì„ ê³ ë ¤í•˜ê¸° ìœ„í•´ ë©”íƒ€ë°ì´í„° ì •ë³´ ë¬´ì‹œ"
      ],
      "metadata": {
        "id": "_FHBG3yOFNVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = common_voice.remove_columns(\n",
        "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\", \"variant\"]\n",
        ")\n",
        "\n",
        "print(common_voice)"
      ],
      "metadata": {
        "id": "3O53cVAsFOPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### íŠ¹ì„± ì¶”ì¶œê¸°(Feature Extractor), í† í¬ë‚˜ì´ì €(Tokenizer), ê·¸ë¦¬ê³  ë°ì´í„°ì¤€ë¹„\n"
      ],
      "metadata": {
        "id": "_ivtrPBAFPaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ASR íŒŒì´í”„ë¼ì¸ ì„¸ ë‹¨ê³„ë¡œ ë¶„í•´:\n",
        "\n",
        "1. Raw ì˜¤ë””ì˜¤ ì…ë ¥ì„ ì „ì²˜ë¦¬í•˜ëŠ” íŠ¹ì • ì¶”ì¶œê¸°\n",
        "2. ì‹œí€€ìŠ¤ ê°„ ë§¤í•‘ì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸\n",
        "3. ëª¨ë¸ ì¶œë ¥ì„ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ í›„ì²˜ë¦¬í•˜ëŠ” tokenizer\n",
        "\n",
        "\n",
        "* Whisper\n",
        "    * [WhisperFeatureExtractor](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperFeatureExtractor)ì™€ [WhisperTokenizer](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperTokenizer)ë¡œ êµ¬ì„±\n",
        "\n"
      ],
      "metadata": {
        "id": "iJXoUM3bFQ1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)"
      ],
      "metadata": {
        "id": "j00MN9qwFRxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperTokenizer\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)"
      ],
      "metadata": {
        "id": "01ArF8lCFSbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* WhisperProcessor í´ë¼ìŠ¤\n",
        "    * íŠ¹ì„± ì¶”ì¶œê¸°ì™€ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë‘ ê°€ì§€ë¥¼ ëª¨ë‘ í•©ì¹©\n",
        "    * í•„ìš”ì— ë”°ë¼ ì˜¤ë””ì˜¤ ì…ë ¥ ë° ëª¨ë¸ ì˜ˆì¸¡ì— ì‚¬ìš© ê°€ëŠ¥\n",
        "\n",
        "* í•™ìŠµ ì¤‘ì— ë‘ ê°œì˜ ê°ì²´ë§Œ ì¶”ì  í•„ìš”: í”„ë¡œì„¸ì„œì™€ ëª¨ë¸"
      ],
      "metadata": {
        "id": "JHEDUSrzFUSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
      ],
      "metadata": {
        "id": "vE4DXwpaFVJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ë°ì´í„° ì¤€ë¹„\n",
        "\n"
      ],
      "metadata": {
        "id": "SSUPah6yFWR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common Voice ë°ì´í„°ì…‹ì˜ ì²« ë²ˆì§¸ ì˜ˆì œë¥¼ ì¶œë ¥í•˜ì—¬ ë°ì´í„°ì˜ í˜•ì‹ì„ ì‚´í´ë´„"
      ],
      "metadata": {
        "id": "N7ub4GcLFYHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(common_voice[\"train\"][0])"
      ],
      "metadata": {
        "id": "La1zJTYRFbmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Whisper ëª¨ë¸ ìƒ˜í”Œë§\n",
        "    * ì…ë ¥ ì˜¤ë””ì˜¤ëŠ” 48 kHz ìƒˆí”Œë§\n",
        "    * Whisper feature extractorì— ì „ë‹¬í•˜ê¸° ìœ„í•´ì„œ 16 kHzë¡œ ë‹¤ìš´ìƒ˜í”Œ ì§„í–‰\n",
        "* ìƒ˜í”Œë§ ì†ë„ ì„¤ì •\n",
        "    * Datasetì˜ [`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column) ë°©ë²• ì‚¬ìš©: ì˜¤ë””ì˜¤ ì…ë ¥ì„ ì˜¬ë°”ë¥¸ ìƒ˜í”Œë§ ì†ë„ë¡œ ì„¤ì •\n",
        "    * ì˜¤ë””ì˜¤ë¥¼ ë³€ê²½í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì˜¤ë””ì˜¤ ìƒ˜í”Œì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆë„ë¡ í•¨\n"
      ],
      "metadata": {
        "id": "3X0TJCD8FegP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Audio\n",
        "\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ],
      "metadata": {
        "id": "W8c-WIpaFfvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Common Voice ë°ì´í„°ì…‹ì—ì„œ ì²« ë²ˆì§¸ ì˜¤ë””ì˜¤ ìƒ˜í”Œì„ ë‹¤ì‹œë¡œë“œí•˜ë©´ ì›í•˜ëŠ” ìƒ˜í”Œë§ ì†ë„ë¡œ ë‹¤ì‹œ ìƒ˜í”Œë§"
      ],
      "metadata": {
        "id": "BMV7Af7SFh-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(common_voice[\"train\"][0])"
      ],
      "metadata": {
        "id": "R8QA_OV1Fi2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* ëª¨ë¸ì— ë§ê²Œ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” í•¨ìˆ˜:\n",
        "\n",
        "1. batch[\"audio\"]ë¥¼ í˜¸ì¶œí•˜ì—¬ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ë‹¤ì‹œ ìƒ˜í”Œë§. ğŸ¤— DatasetsëŠ” í•„ìš”í•œ ëª¨ë“  ì¬ìƒ˜í”Œë§ ì‘ì—…ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜í–‰\n",
        "3. Feature extractorë¥¼ ì‚¬ìš©í•˜ì—¬ 1ì°¨ì› ì˜¤ë””ì˜¤ ë°°ì—´ì—ì„œ ë¡œê·¸ ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì…ë ¥ íŠ¹ì„±ì„ ê³„ì‚°\n",
        "3. Tokenizerë¥¼ ì‚¬ìš©í•˜ì—¬ transcriptsë¥¼ ë ˆì´ë¸” idsë¡œ ì¸ì½”ë”©\n"
      ],
      "metadata": {
        "id": "R1YL5hcvFkU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids\n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch"
      ],
      "metadata": {
        "id": "bHnrXcSoFltS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜\n",
        "    * ë°ì´í„°ì…‹ì˜ .map ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  í•™ìŠµ ì˜ˆì œì— ì ìš© ê°€ëŠ¥\n",
        "    * num_proc: ëª‡ ê°œì˜ CPU ì½”ì–´ë¥¼ ì‚¬ìš©í•  ì§€ë¥¼ ì§€ì •,num_procë¥¼ 1ë³´ë‹¤ í¬ê²Œ ì„¤ì •í•˜ë©´ ë‹¤ì¤‘ ì²˜ë¦¬ê°€ í™œì„±í™” (ë‹¤ì¤‘ ì²˜ë¦¬ë¡œ .map ë©”ì„œë“œê°€ ì¤‘ë‹¨ë˜ëŠ” ê²½ìš° num_proc=1ë¡œ ì„¤ì •í•˜ê³  ë°ì´í„°ì…‹ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬)\n",
        "\n",
        "* Datasetì˜ ì‚¬ì´ì¦ˆì— ë”°ë¼ì„œ 20~30 ë¶„ ì •ë„ ê±¸ë¦¼\n"
      ],
      "metadata": {
        "id": "qyenTQcQFmz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2)"
      ],
      "metadata": {
        "id": "QZx_aOpJFoO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice[\"train\"]"
      ],
      "metadata": {
        "id": "tr7R7OtcFpeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### í•™ìŠµ ë° ê²€ì¦\n"
      ],
      "metadata": {
        "id": "ZvXDlAlVFq1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* í›ˆë ¨ íŒŒì´í”„ë¼ì¸\n",
        "* [ğŸ¤— Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer)ê°€ ëŒ€ë¶€ë¶„ì˜ ì‘ì—…ì„ ì²˜ë¦¬:\n",
        "\n",
        "\n",
        "1. ë°ì´í„° collator ì •ì˜: ë°ì´í„° ì½œë ˆì´í„°ëŠ” ìš°ë¦¬ê°€ ì „ì²˜ë¦¬í•œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ëª¨ë¸ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” PyTorch í…ì„œë¡œ ì¤€ë¹„\n",
        "2. í‰ê°€ ì§€í‘œ: í‰ê°€ ì¤‘ì—ëŠ” ëª¨ë¸ì„ ê¸€ì ì˜¤ë¥˜ìœ¨  [word error rate (CER)](https://huggingface.co/metrics/cer)ì§€í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€\n",
        "3. ì‚¬ì „ í›ˆë ¨ëœ ì²´í¬í¬ì¸íŠ¸ load: ì‚¬ì „ í›ˆë ¨ëœ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•˜ê³  í›ˆë ¨ì„ ìœ„í•´ ì˜¬ë°”ë¥´ê²Œ êµ¬ì„±\n",
        "4. í›ˆë ¨ êµ¬ì„± ì •ì˜: ğŸ¤— Trainerê°€ í›ˆë ¨ ìŠ¤ì¼€ì¤„ì„ ì •ì˜ì— ì‚¬ìš©\n",
        "\n",
        "* ë¯¸ì„¸ ì¡°ì •í•œ í›„ì—ëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ëª¨ë¸ì„ í‰ê°€í•˜ì—¬ í•œêµ­ì–´ ìŒì„±ì„ ì˜¬ë°”ë¥´ê²Œ transcribe\n"
      ],
      "metadata": {
        "id": "Oun5Wnc2FsPH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Collator ì •ì˜\n"
      ],
      "metadata": {
        "id": "KBITeAvgFt44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ ìŒì„± ëª¨ë¸ì˜ ë°ì´í„° ì½œë ˆì´í„°\n",
        "    * Input_featuresì™€ labelsë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ ì°¨ë³„í™”\n",
        "    \n",
        "    - Input_features: feature extractorì— ì˜í•´ ì²˜ë¦¬\n",
        "    - labels: tokenizerì— ì˜í•´ ì²˜ë¦¬\n",
        "\n",
        "* Input_featuresëŠ” ì´ë¯¸ 30ì´ˆë¡œ íŒ¨ë”©ë˜ì–´ ìˆê³  íŠ¹ì„± ì¶”ì¶œê¸°ì— ì˜í•´ ê³ ì •ëœ ì°¨ì›ì˜ ë¡œê·¸ ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ìœ¼ë¡œ ë³€í™˜. ë”°ë¼ì„œ ìš°ë¦¬ê°€ í•´ì•¼ í•  ì¼ì€ input_featuresë¥¼ ë°°ì¹˜ ì²˜ë¦¬ëœ PyTorch í…ì„œë¡œ ë³€í™˜\n",
        "\n",
        "* labelsëŠ” íŒ¨ë”©ë˜ì§€ ì•ŠìŒ ë¨¼ì € ë°°ì¹˜ ë‚´ì—ì„œ ìµœëŒ€ ê¸¸ì´ì— ë§ê²Œ ì‹œí€€ìŠ¤ë¥¼ íŒ¨ë”©í•˜ê³ , tokenizerì˜ .pad ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œí€€ìŠ¤ë¥¼ íŒ¨ë”© íŒ¨ë”© í† í°ì€ ì†ì‹¤ì„ ê³„ì‚°í•  ë•Œ ê³ ë ¤ë˜ì§€ ì•Šë„ë¡ -100ìœ¼ë¡œ ëŒ€ì²´. ê·¸ëŸ° ë‹¤ìŒ ë ˆì´ë¸” ì‹œí€€ìŠ¤ì˜ ì‹œì‘ì—ì„œ BOS í† í°ì„ ì˜ë¼ì„œ í›ˆë ¨ ì¤‘ì— ë‚˜ì¤‘ì— ì´ë¥¼ ì¶”ê°€\n",
        "\n",
        "* ì´ì „ì— ì •ì˜í•œ WhisperProcessorë¥¼ í™œìš©í•˜ì—¬ íŠ¹ì„± ì¶”ì¶œê¸° ë° í† í¬ë‚˜ì´ì € ì‘ì—…ì„ ëª¨ë‘ ìˆ˜í–‰ ê°€ëŠ¥"
      ],
      "metadata": {
        "id": "umBjrMi9FviZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "RPDj2653Fwt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data collator ì´ˆê¸°í™” ì§„í–‰"
      ],
      "metadata": {
        "id": "vnUqIKCRFyAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ],
      "metadata": {
        "id": "2BeOoSfAFyxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### í‰ê°€ ì§€í‘œ"
      ],
      "metadata": {
        "id": "Rc1Ovn_iFzso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* ASR ì‹œìŠ¤í…œì„ í‰ê°€í•˜ê¸° ìœ„í•œ 'ì‚¬ì‹¤ìƒì˜' ì§€í‘œì¸ í•œ ë‹¨ì–´ ì˜¤ë¥˜ìœ¨(CER) ë©”íŠ¸ë¦­ì„ ì‚¬ìš©\n",
        "* ë” ë§ì€ ì •ë³´ëŠ” [ë¬¸ì„œ](https://huggingface.co/metrics/cer)ë¥¼ ì°¸ì¡°. ìš°ë¦¬ëŠ” ğŸ¤— Evaluateì—ì„œ CER ë©”íŠ¸ë¦­ì„ ë¡œë“œ"
      ],
      "metadata": {
        "id": "76KMFfvfF0td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"cer\")"
      ],
      "metadata": {
        "id": "D3of2K85F1Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Â Pre-trained ëª¨ë¸ ë¡œë“œ"
      ],
      "metadata": {
        "id": "TklAg4VeF2Wt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ì‚¬ì „ í›ˆë ¨ëœ Whisper ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œ\n",
        "    * ì´ ì‘ì—…ì€ ğŸ¤— Transformersë¥¼ ì‚¬ìš©í•˜ì—¬ ë§¤ìš° ê°„ë‹¨\n",
        "\n",
        "\n",
        "\n",
        "* ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ ëª¨ë¸ì„ 8ë¹„íŠ¸ë¡œ         \n",
        "    * ëª¨ë¸ì„ 1/4 ì •ë°€ë„(32ë¹„íŠ¸ì™€ ë¹„êµí–ˆì„ ë•Œ)ë¡œ ì–‘ìí™”í•˜ì—¬ ì„±ëŠ¥ ì†ì‹¤ì„ ìµœì†Œí™” [here](https://huggingface.co/blog/hf-bitsandbytes-integration)"
      ],
      "metadata": {
        "id": "12EdQdpuF3yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, load_in_8bit=True, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "3Acl29i1F42G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ëª¨ë¸ì˜ í›„ì²˜ë¦¬"
      ],
      "metadata": {
        "id": "URrUdimqF6TP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. í›ˆë ¨ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ê¸° ìœ„í•´ 8ë¹„íŠ¸ ëª¨ë¸ì— ëª‡ ê°€ì§€ í›„ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ì ìš©\n",
        "2. ëª¨ë¸ ë ˆì´ì–´ë¥¼ ë™ê²°, í›ˆë ¨ê³¼ ëª¨ë¸ì˜ ì•ˆì •ì„±ì„ ìœ„í•´ ë ˆì´ì–´ ì •ê·œí™”ì™€ ì¶œë ¥ ë ˆì´ì–´ë¥¼ float32ë¡œ ìºìŠ¤íŒ…\n",
        "\n",
        "(ëª¨ë¸ ì•ˆì •ì„±ê³¼ layer normalization ë¶„ì„, float32ë¡œ ìºìŠ¤íŒ… í•˜ëŠ” ì´ìœ )"
      ],
      "metadata": {
        "id": "oKj9unKWF7c6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "id": "qChHa0ptF8eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model = prepare_model_for_kbit_training(model, output_embedding_layer_name=\"proj_out\")"
      ],
      "metadata": {
        "id": "c6CieDF-F9TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Whisper ëª¨ë¸ì€ ì¸ì½”ë”ì— ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì²´í¬í¬ì¸íŒ…ì€ grad ì—°ì‚°ì„ ë¹„í™œì„±. ì´ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì…ë ¥ì„ íŠ¹ë³„íˆ trainableí•˜ê²Œ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.\n"
      ],
      "metadata": {
        "id": "tNpShzrUF-Pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_inputs_require_grad(module, input, output):\n",
        "    output.requires_grad_(True)\n",
        "\n",
        "model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)"
      ],
      "metadata": {
        "id": "D_XQsrQ6F_GH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Low-rank adapters (LoRA)ë¥¼ ëª¨ë¸ì— ì ìš©\n"
      ],
      "metadata": {
        "id": "SA03MdIkGAS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* peftì—ì„œ get_peft_model ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ PeftModelì„ ë¡œë“œí•˜ê³  ì €í¬ê°€ ì €ì°¨ì› ì–´ëŒ‘í„°(LoRA)ë¥¼ ì‚¬ìš©í•  ê²ƒì„ì„ ì§€ì •\n"
      ],
      "metadata": {
        "id": "PvwkztSoGBiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "dwMYIPnYGCyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1%**ì˜ í•™ìŠµ parameterë¥¼ ì‚¬ìš©í•˜ì˜€ê³  **Parameter-Efficient Fine-Tuning**ë¥¼ ì ìš©\n"
      ],
      "metadata": {
        "id": "mkOvUzL5GDw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### í›ˆë ¨ êµ¬ì„± ì •ì˜"
      ],
      "metadata": {
        "id": "jJNqmo2lGE1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ë§ˆì§€ë§‰ ë‹¨ê³„ì—ì„œëŠ” í›ˆë ¨ê³¼ ê´€ë ¨ëœ ëª¨ë“  ë§¤ê°œë³€ìˆ˜ë¥¼ ì •ì˜ í›ˆë ¨ ì¸ìì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ í•´ë‹¹ ë¬¸ì„œë¥¼ ì°¸ì¡° Seq2SeqTrainingArguments [docs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments)\n"
      ],
      "metadata": {
        "id": "CGwnUXB4GHhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"reach-vb/test\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-3,\n",
        "    warmup_steps=50,\n",
        "    num_train_epochs=1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    fp16=True,\n",
        "    per_device_eval_batch_size=8,\n",
        "    generation_max_length=128,\n",
        "    logging_steps=100,\n",
        "    max_steps=100, # only for testing purposes, remove this from your final run :)\n",
        "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
        "    label_names=[\"labels\"],  # same reason as above\n",
        ")"
      ],
      "metadata": {
        "id": "7qPc07lFGG9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* PEFTë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒì—ëŠ” ëª‡ ê°€ì§€ ì£¼ì˜ê°€ í•„ìš”\n",
        "\n",
        "1. PeftModelì˜ forwardê°€ ê¸°ë³¸ ëª¨ë¸ì˜ forwardì˜ ì‹œê·¸ë‹ˆì²˜ë¥¼ ìƒì†í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— remove_unused_columns=False ë° label_names=[\"labels\"]ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •\n",
        "2. INT8 í›ˆë ¨ì—ëŠ” ìë™ ìºìŠ¤íŒ…ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— Trainerì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ ì œê³µë˜ëŠ” predict_with_generate í˜¸ì¶œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìë™ ìºìŠ¤íŒ…ì´ ìë™ìœ¼ë¡œ ì ìš©ë˜ì§€ ì•ŠìŒ\n",
        "3. ìë™ ìºìŠ¤íŒ…ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ Seq2SeqTrainerì— compute_metricsë¥¼ ì „ë‹¬í•  ìˆ˜ ì—†ìŒ. ë”°ë¼ì„œ Trainerë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ëŠ” ë™ì•ˆ compute_metricsë¥¼ ì£¼ì„ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n"
      ],
      "metadata": {
        "id": "jd9TNUakGJhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
        "\n",
        "# This callback helps to save only the adapter weights and remove the base model weights.\n",
        "class SavePeftModelCallback(TrainerCallback):\n",
        "    def on_save(\n",
        "        self,\n",
        "        args: TrainingArguments,\n",
        "        state: TrainerState,\n",
        "        control: TrainerControl,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
        "\n",
        "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
        "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
        "\n",
        "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
        "        if os.path.exists(pytorch_model_path):\n",
        "            os.remove(pytorch_model_path)\n",
        "        return control\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=common_voice[\"train\"],\n",
        "    eval_dataset=common_voice[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    # compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    callbacks=[SavePeftModelCallback],\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ],
      "metadata": {
        "id": "mlxRmXiGGKs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "jc2hSblfGL9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our model is fine-tuned, we can push the model on to Hugging Face Hub, this will later help us directly infer the model from the model repo."
      ],
      "metadata": {
        "id": "N_y3dUucGM6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_id = \"reach-vb/whisper-large-v2-hindi-100steps\"\n",
        "model.push_to_hub(peft_model_id)"
      ],
      "metadata": {
        "id": "l2SCSgBQGOBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation and Inference"
      ],
      "metadata": {
        "id": "K79J0U3EGPHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On to the fun part, we've successfully fine-tuned our model. Now let's put it to test and calculate the WER on the `test` set.\n",
        "\n",
        "As with training, we do have a few caveats to pay attention to:\n",
        "1. Since we cannot use `predict_with_generate` function, we will hand roll our own eval loop with `torch.cuda.amp.autocast()` you can check it out below.\n",
        "2. Since the base model is frozen, PEFT model sometimes fails to recognise the language while decoding. To fix that, we force the starting tokens to mention the language we are transcribing. This is done via `forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"Marathi\", task=\"transcribe\")` and passing that too the `model.generate` call.\n",
        "\n",
        "That's it, let's get transcribing! ğŸ”¥\n"
      ],
      "metadata": {
        "id": "-q4B_OCYGQev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer\n",
        "\n",
        "peft_model_id = \"reach-vb/whisper-large-v2-hindi-100steps\" # Use the same model ID as before.\n",
        "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\n",
        "    peft_config.base_model_name_or_path, load_in_8bit=True, device_map=\"auto\"\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "model.config.use_cache = True"
      ],
      "metadata": {
        "id": "s2kHJxk1GTWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
        "\n",
        "eval_dataloader = DataLoader(common_voice[\"test\"], batch_size=8, collate_fn=data_collator)\n",
        "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n",
        "normalizer = BasicTextNormalizer()\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "normalized_predictions = []\n",
        "normalized_references = []\n",
        "\n",
        "model.eval()\n",
        "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
        "    with torch.cuda.amp.autocast():\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = (\n",
        "                model.generate(\n",
        "                    input_features=batch[\"input_features\"].to(\"cuda\"),\n",
        "                    forced_decoder_ids=forced_decoder_ids,\n",
        "                    max_new_tokens=255,\n",
        "                )\n",
        "                .cpu()\n",
        "                .numpy()\n",
        "            )\n",
        "            labels = batch[\"labels\"].cpu().numpy()\n",
        "            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
        "            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "            predictions.extend(decoded_preds)\n",
        "            references.extend(decoded_labels)\n",
        "            normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n",
        "            normalized_references.extend([normalizer(label).strip() for label in decoded_labels])\n",
        "        del generated_tokens, labels, batch\n",
        "    gc.collect()\n",
        "wer = 100 * metric.compute(predictions=predictions, references=references)\n",
        "normalized_wer = 100 * metric.compute(predictions=normalized_predictions, references=normalized_references)\n",
        "eval_metrics = {\"eval/wer\": wer, \"eval/normalized_wer\": normalized_wer}\n",
        "\n",
        "print(f\"{wer=} and {normalized_wer=}\")\n",
        "print(eval_metrics)"
      ],
      "metadata": {
        "id": "Y55VK-B0GUIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fin!\n"
      ],
      "metadata": {
        "id": "9Z9qEpEGGS13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "If you made it all the way till the end then pat yourself on the back. Looking back, we learned how to train *any* Whisper checkpoint faster, cheaper and with negligible loss in WER.\n",
        "\n",
        "With PEFT, you can also go beyond Speech recognition and apply the same set of techniques to other pretrained models as well. Come check it out here: https://github.com/huggingface/peft ğŸ¤—\n",
        "\n",
        "Don't forget to tweet your results and tag us! [@huggingface](https://twitter.com/huggingface) and [@reach_vb](https://twitter.com/reach_vb) â¤ï¸"
      ],
      "metadata": {
        "id": "UkZWOMr3GXXJ"
      }
    }
  ]
}