{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN/08id6yka0hubrIzpIk0k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoungsikMoon/FORS/blob/main/%EC%A1%B0%EC%98%81%EC%88%98/FORS_%EC%A1%B0%EC%98%81%EC%88%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Whisper AI fine-tuning 하는 코드"
      ],
      "metadata": {
        "id": "Bk4cF7T2ApIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WhisperAI_fine_tune_version6"
      ],
      "metadata": {
        "id": "-zKatACsAq_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRA (Low Rank Adaptation) 와 PEFT (Paramater Efficient Fine Tuning)를 적용한 Larger Whisper fine-tuning ⚡️\n"
      ],
      "metadata": {
        "id": "YyhW-6uLAwmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* 소비자 GPU의 VRAM이 8GB 미만인 환경에서도 full-finetuning과 유사한 성능을 제공함\n",
        "*  🤗 Transformers and PEFT 모델과 Common Voice 13.0 dataset를 사용하여 Whisper fine-tune하는 과정 설명함\n",
        "* PEFT와 bitsandbytes를 활용하여 무료 T4 GPU(16GB VRAM)를 사용하여 whisper-large-v2 체크포인트를 원활하게 학습"
      ],
      "metadata": {
        "id": "GI9CES8OCw5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 왜 Parameter Efficient Fine Tuning [PEFT](https://github.com/huggingface/peft)를 사용해야 되는가?\n"
      ],
      "metadata": {
        "id": "AwOEdVeIA0gR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 모델 사이즈 증가로 fine tuning하는 것이 계산 복잡성 증가와 메모리 사용량 증가\n",
        "    * 예를 들어, Whisper-large-v2 모델을 완전한 미세 조정을 위해 약 24GB의 GPU VRAM이 필요하며, 각 미세 조정된 모델은 약 7GB의 저장 공간을 필요함\n",
        "\n",
        "    * 제한적인 환경에서 bottleneck 발생하고 원하는 결과를 얻기 힘듦\n",
        "* PEFT\n",
        "    * 효과적으로 parameter를 줄여서 fine tuning 속도 개선\n",
        "    * 목적: 병목 현상을 해결\n",
        "    * 접근법(예: 저수준 적응): 사전 훈련된 모델의 대부분의 매개변수를 동결시키면서 추가 모델 매개변수의 일부만 미세 조정하여 계산 및 저장 비용 크게 줄임\n",
        "        * 대규모 모델의 전체 미세 조정 중 관찰되는 catastrophic forgetting 문제를 극복할 수 있음\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1-qDk6vUA2DX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LoRA가 무엇인가?\n"
      ],
      "metadata": {
        "id": "hjtgo0vBA7tL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* PEFT에서 여러 매개변수 효율적인 기술을 기본으로 제공\n",
        "    * 그 중 하나인 Low Rank Adaptation (LoRA)\n",
        "        * 사전 훈련된 모델 가중치를 동결하고 Transformer 아키텍처의 각 레이어에 훈련 가능한 랭크 분해 행렬을 삽입 (High Rank 즉 많은 연결이 되어 있는 것들보다 연결이 적은 Low Rank로 만들어서 계산량을 줄임)\n",
        "            * Downstream 작업에 대한 훈련 가능한 매개변수 수가 크게 감소\n"
      ],
      "metadata": {
        "id": "_IqBnkBlC0em"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 통계로 보는 PEFT 효과\n"
      ],
      "metadata": {
        "id": "20Ii_PmCBEZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Full fine-tuning of Whisper-large-v2 checkpoint Vs. PEFT 적용 모델\n",
        "\n",
        "    1. GPU VRAM이 8GB 미만인 환경에서 16억 개의 매개변수를 가진 모델을 미세 조정 🤯\n",
        "    2. 훨씬 적은 수의 훈련 가능한 매개변수를 사용하여 거의 5배 더 큰 배치 크기를 사용 가능 📈\n",
        "    3. 생성된 체크포인트는 원본 모델의 크기의 1%인 약 60MB 🚀\n",
        "* 기존 🤗 transformers Whisper에서 변형이 많이 되지 않았음"
      ],
      "metadata": {
        "id": "Wg0LhH75C6HU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 환경설정\n"
      ],
      "metadata": {
        "id": "MiCJiRtgBHYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* Python package->Whisper 모델 fine tuning하기 위해 사용\n",
        "  * `datasets`:학습 데이터를 다운로드하고 준비\n",
        "  * `transformers`: Whisper 모델을 로드하고 훈련\n",
        "  * `librosa`: 오디오 파일을 전처리\n",
        "  * `evaluate` &  `jiwer`:모델의 성능을 평가\n",
        "  * `PEFT`, `bitsandbytes`, `accelerate`: LoRA로 모델과 fine-tuning"
      ],
      "metadata": {
        "id": "Nqb9770SC8Gz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sACxsKfCAdZo"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets librosa evaluate jiwer gradio bitsandbytes==0.37 accelerate\n",
        "!pip install -q git+https://github.com/huggingface/peft.git@main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install typer==0.9.1"
      ],
      "metadata": {
        "id": "Rlp5jkcKBKHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* GPU 확보\n",
        "    * Google Colab Pro: V100 또는 P100 GPU가 할당\n",
        "* GPU 확보 방법\n",
        "    * 런타임 -> 런타임 유형 변경\n",
        "    * None -> GPU 변경\n",
        "* GPU 할당 확인"
      ],
      "metadata": {
        "id": "fhZiYqOvBLxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "78HJfwwgBNTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab에서 제공하는 GPU사용"
      ],
      "metadata": {
        "id": "kGYOX3puBO5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "N8VwGP0WBQPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We strongly advise you to upload model checkpoints directly the [Hugging Face Hub](https://huggingface.co/)\n",
        "whilst training. The Hub provides:\n",
        "- Integrated version control: you can be sure that no model checkpoint is lost during training.\n",
        "- Tensorboard logs: track important metrics over the course of training.\n",
        "- Model cards: document what a model does and its intended use cases.\n",
        "- Community: an easy way to share and collaborate with the community!\n",
        "\n",
        "\n",
        "* 학습 중 모델 체크포인트를 직접 Hugging Face Hub에 업로드하는 것을 강력히 권장\n",
        " * Hub를 사용하면 다음과 같은 기능을 제공:\n",
        "\n",
        " 1. 통합된 버전 관리: 훈련 중에 어떠한 모델 체크포인트도 손실되지 않음\n",
        " 2. Tensorboard 로그: 훈련 과정에서 중요한 지표를 추적\n",
        " 3. 모델 카드: 모델이 무엇을 하고 의도된 사용 사례를 문서화\n",
        " 4. 커뮤니티: 커뮤니티와 쉽게 공유하고 협업\n",
        "\n",
        "* Hub에 연결\n",
        " * 프롬프트에서 Hub 인증 토큰을 입력:"
      ],
      "metadata": {
        "id": "97s-5e1_BRa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "7PXhqHnPBUAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Whisper 모델 checkpoint와 task 설정"
      ],
      "metadata": {
        "id": "-IxwKzYUBVTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"openai/whisper-large-v2\"\n",
        "task = \"transcribe\""
      ],
      "metadata": {
        "id": "ZF_Yzhh8BYYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터셋 디테일을 설정 (언어)"
      ],
      "metadata": {
        "id": "mPaxRfLnBZ0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"mozilla-foundation/common_voice_13_0\"\n",
        "language = \"Korean\"\n",
        "language_abbr = \"ko\" # Short hand code for the language we want to fine-tune"
      ],
      "metadata": {
        "id": "SKFpfEuZBb4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터셋 올리기\n"
      ],
      "metadata": {
        "id": "6VEIzD2tBeW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Huggingface Dataset 사용\n",
        " * 적은 코드로 Common Voice의 데이터를 다운로드하고 준비\n",
        "\n",
        "* 확인 절차\n",
        " 1. Hugging Face Hub의 이용 약관을 수락했는지 확인:[mozilla-foundation/common_voice_13_0](https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0)\n",
        " 2. 데이터셋에 액세스하고 로컬로 데이터를 다운로드\n",
        "\n",
        "* 학습+검증 데이터셋/테스트 데이터셋 분리"
      ],
      "metadata": {
        "id": "GkDJv-QgDARH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice[\"train\"] = load_dataset(dataset_name, language_abbr, split=\"train+validation\", use_auth_token=True)\n",
        "common_voice[\"test\"] = load_dataset(dataset_name, language_abbr, split=\"test\", use_auth_token=True)\n",
        "\n",
        "print(common_voice)"
      ],
      "metadata": {
        "id": "nbYnyTYtBgL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 일반적인 ASR(음성 인식) 데이터셋\n",
        "    * 입력 오디오 샘플(오디오)과 해당되는 텍스트(문장)만 제공\n",
        "* Common Voice\n",
        "    * ASR에는 필요하지 않은 악센트와 로케일과 같은 추가 메타데이터 정보가 포함\n",
        "    * 일반적인 용도로 사용하고 미세 조정을 고려하기 위해 메타데이터 정보 무시"
      ],
      "metadata": {
        "id": "S3EHxQ4UBhiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = common_voice.remove_columns(\n",
        "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\", \"variant\"]\n",
        ")\n",
        "\n",
        "print(common_voice)"
      ],
      "metadata": {
        "id": "_nohoiLhBldK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 특성 추출기(Feature Extractor), 토크나이저(Tokenizer), 그리고 데이터준비\n"
      ],
      "metadata": {
        "id": "Q0KdlbRhBlCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ASR 파이프라인 세 단계로 분해:\n",
        "\n",
        "1. Raw 오디오 입력을 전처리하는 특정 추출기\n",
        "2. 시퀀스 간 매핑을 수행하는 모델\n",
        "3. 모델 출력을 텍스트 형식으로 후처리하는 tokenizer\n",
        "\n",
        "\n",
        "* Whisper\n",
        "    * [WhisperFeatureExtractor](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperFeatureExtractor)와 [WhisperTokenizer](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperTokenizer)로 구성\n",
        "\n"
      ],
      "metadata": {
        "id": "tZaogWp9DCYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)"
      ],
      "metadata": {
        "id": "is9p-WzEBpXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperTokenizer\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)"
      ],
      "metadata": {
        "id": "j0e_p8K2BqMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* WhisperProcessor 클라스\n",
        "    * 특성 추출기와 토크나이저를 사용하기 위해 두 가지를 모두 합칩\n",
        "    * 필요에 따라 오디오 입력 및 모델 예측에 사용 가능\n",
        "\n",
        "* 학습 중에 두 개의 객체만 추적 필요: 프로세서와 모델"
      ],
      "metadata": {
        "id": "ARa3LohKBsZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
      ],
      "metadata": {
        "id": "4UJ-dMpABtce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 데이터 준비\n"
      ],
      "metadata": {
        "id": "Gpa0LrsKBvCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Common Voice 데이터셋의 첫 번째 예제를 출력하여 데이터의 형식을 살펴봄"
      ],
      "metadata": {
        "id": "kssVueqoDEaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(common_voice[\"train\"][0])"
      ],
      "metadata": {
        "id": "4C-Qaz03Bxl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Whisper 모델 샘플링\n",
        "    * 입력 오디오는 48 kHz 새플링\n",
        "    * Whisper feature extractor에 전달하기 위해서 16 kHz로 다운샘플 진행\n",
        "* 샘플링 속도 설정\n",
        "    * Dataset의 [`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column) 방법 사용: 오디오 입력을 올바른 샘플링 속도로 설정\n",
        "    * 오디오를 변경하는 것이 아니라 오디오 샘플을 실시간으로 받을 수 있도록 함\n"
      ],
      "metadata": {
        "id": "H3ykq2UrBypT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Audio\n",
        "\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ],
      "metadata": {
        "id": "MmchcYGjB0C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Common Voice 데이터셋에서 첫 번째 오디오 샘플을 다시로드하면 원하는 샘플링 속도로 다시 샘플링"
      ],
      "metadata": {
        "id": "_DrX-smNB3kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(common_voice[\"train\"][0])"
      ],
      "metadata": {
        "id": "rjqLt6R3B4VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* 모델에 맞게 데이터를 준비하는 함수:\n",
        "\n",
        "1. batch[\"audio\"]를 호출하여 오디오 데이터를 로드하고 다시 샘플링. 🤗 Datasets는 필요한 모든 재샘플링 작업을 실시간으로 수행\n",
        "3. Feature extractor를 사용하여 1차원 오디오 배열에서 로그 멜 스펙트로그램 입력 특성을 계산\n",
        "3. Tokenizer를 사용하여 transcripts를 레이블 ids로 인코딩\n"
      ],
      "metadata": {
        "id": "8WgVIxR_B5Sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids\n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch"
      ],
      "metadata": {
        "id": "NRcYfrHlB615"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 데이터 준비 함수\n",
        "    * 데이터셋의 .map 메서드를 사용하여 모든 학습 예제에 적용 가능\n",
        "    * num_proc: 몇 개의 CPU 코어를 사용할 지를 지정,num_proc를 1보다 크게 설정하면 다중 처리가 활성화 (다중 처리로 .map 메서드가 중단되는 경우 num_proc=1로 설정하고 데이터셋을 순차적으로 처리)\n",
        "\n",
        "* Dataset의 사이즈에 따라서 20~30 분 정도 걸림\n"
      ],
      "metadata": {
        "id": "HFeLximJB8My"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2)"
      ],
      "metadata": {
        "id": "0WRDWCmhB-S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice[\"train\"]"
      ],
      "metadata": {
        "id": "5P3MOMOJB_HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 및 검증\n",
        "\n"
      ],
      "metadata": {
        "id": "cTzGBVYvCAaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* 훈련 파이프라인\n",
        "* [🤗 Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer)가 대부분의 작업을 처리:\n",
        "\n",
        "\n",
        "1. 데이터 collator 정의: 데이터 콜레이터는 우리가 전처리한 데이터를 가져와 모델에 사용할 수 있는 PyTorch 텐서로 준비\n",
        "2. 평가 지표: 평가 중에는 모델을 글자 오류율  [word error rate (CER)](https://huggingface.co/metrics/cer)지표를 사용하여 평가\n",
        "3. 사전 훈련된 체크포인트 load: 사전 훈련된 체크포인트를 로드하고 훈련을 위해 올바르게 구성\n",
        "4. 훈련 구성 정의: 🤗 Trainer가 훈련 스케줄을 정의에 사용\n",
        "\n",
        "* 미세 조정한 후에는 테스트 데이터에서 모델을 평가하여 한국어 음성을 올바르게 transcribe"
      ],
      "metadata": {
        "id": "WE2_imGHDHM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Collator 정의\n"
      ],
      "metadata": {
        "id": "1HoMXTkCCEU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* 시퀀스-투-시퀀스 음성 모델의 데이터 콜레이터\n",
        "    * Input_features와 labels를 독립적으로 처리 차별화\n",
        "    \n",
        "    - Input_features: feature extractor에 의해 처리\n",
        "    - labels: tokenizer에 의해 처리\n",
        "\n",
        "* Input_features는 이미 30초로 패딩되어 있고 특성 추출기에 의해 고정된 차원의 로그 멜 스펙트로그램으로 변환. 따라서 우리가 해야 할 일은 input_features를 배치 처리된 PyTorch 텐서로 변환\n",
        "\n",
        "* labels는 패딩되지 않음 먼저 배치 내에서 최대 길이에 맞게 시퀀스를 패딩하고, tokenizer의 .pad 메서드를 사용하여 시퀀스를 패딩 패딩 토큰은 손실을 계산할 때 고려되지 않도록 -100으로 대체. 그런 다음 레이블 시퀀스의 시작에서 BOS 토큰을 잘라서 훈련 중에 나중에 이를 추가\n",
        "\n",
        "* 이전에 정의한 WhisperProcessor를 활용하여 특성 추출기 및 토크나이저 작업을 모두 수행 가능"
      ],
      "metadata": {
        "id": "JSYUN_-CDJul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "wX8T4B4qCGJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data collator 초기화 진행"
      ],
      "metadata": {
        "id": "WRDNGiVxCICZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ],
      "metadata": {
        "id": "EpyF337cCIvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 평가 지표"
      ],
      "metadata": {
        "id": "Izc2ZJQhCJ-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* ASR 시스템을 평가하기 위한 '사실상의' 지표인 한 단어 오류율(CER) 메트릭을 사용\n",
        "* 더 많은 정보는 [문서](https://huggingface.co/metrics/cer)를 참조. 우리는 🤗 Evaluate에서 CER 메트릭을 로드"
      ],
      "metadata": {
        "id": "PPyFwm9eCLvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"cer\")"
      ],
      "metadata": {
        "id": "q_5KaaC8CMvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pre-trained 모델 로드"
      ],
      "metadata": {
        "id": "zehRf78NCOkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 사전 훈련된 Whisper 체크포인트를 로드\n",
        "    * 이 작업은 🤗 Transformers를 사용하여 매우 간단\n",
        "\n",
        "\n",
        "\n",
        "* 모델의 메모리 사용량을 줄이기 위해 모델을 8비트로         \n",
        "    * 모델을 1/4 정밀도(32비트와 비교했을 때)로 양자화하여 성능 손실을 최소화 [here](https://huggingface.co/blog/hf-bitsandbytes-integration)"
      ],
      "metadata": {
        "id": "8qJm2PvhCQiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, load_in_8bit=True, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "ivkkK8BMCRsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 모델의 후처리\n"
      ],
      "metadata": {
        "id": "K5-OWiwPCSyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. 훈련을 가능하게 하기 위해 8비트 모델에 몇 가지 후처리 단계를 적용\n",
        "2. 모델 레이어를 동결, 훈련과 모델의 안정성을 위해 레이어 정규화와 출력 레이어를 float32로 캐스팅\n",
        "\n",
        "(모델 안정성과 layer normalization 분석, float32로 캐스팅 하는 이유)"
      ],
      "metadata": {
        "id": "pvH50cFLDMlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "id": "CiWVYCsPCULa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model = prepare_model_for_kbit_training(model, output_embedding_layer_name=\"proj_out\")"
      ],
      "metadata": {
        "id": "I_DmsXAECU0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Whisper 모델은 인코더에 컨볼루션 레이어를 사용하기 때문에 체크포인팅은 grad 연산을 비활성. 이를 피하기 위해 입력을 특별히 trainable하게 만들어야 합니다.\n"
      ],
      "metadata": {
        "id": "JENjOj3fCWNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_inputs_require_grad(module, input, output):\n",
        "    output.requires_grad_(True)\n",
        "\n",
        "model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)"
      ],
      "metadata": {
        "id": "HfyVl7UqCXYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Low-rank adapters (LoRA)를 모델에 적용\n"
      ],
      "metadata": {
        "id": "vPPjkWEICYgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* peft에서 get_peft_model 유틸리티 함수를 사용하여 PeftModel을 로드하고 저희가 저차원 어댑터(LoRA)를 사용할 것임을 지정\n"
      ],
      "metadata": {
        "id": "TZof0_YTDOQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "-RHY3bHQCaNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1%**의 학습 parameter를 사용하였고 **Parameter-Efficient Fine-Tuning**를 적용\n"
      ],
      "metadata": {
        "id": "ochIz600Cbd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 훈련 구성 정의"
      ],
      "metadata": {
        "id": "3xZgpcjoCcv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "마지막 단계에서는 훈련과 관련된 모든 매개변수를 정의 훈련 인자에 대한 자세한 내용은 해당 문서를 참조 Seq2SeqTrainingArguments [docs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments)\n"
      ],
      "metadata": {
        "id": "qBStvREQCd4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"reach-vb/test\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-3,\n",
        "    warmup_steps=50,\n",
        "    num_train_epochs=1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    fp16=True,\n",
        "    per_device_eval_batch_size=8,\n",
        "    generation_max_length=128,\n",
        "    logging_steps=100,\n",
        "    max_steps=100, # only for testing purposes, remove this from your final run :)\n",
        "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
        "    label_names=[\"labels\"],  # same reason as above\n",
        ")"
      ],
      "metadata": {
        "id": "8bAfxwiKCeo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* PEFT를 사용하여 모델을 미세 조정하는 것에는 몇 가지 주의가 필요\n",
        "\n",
        "1. PeftModel의 forward가 기본 모델의 forward의 시그니처를 상속하지 않기 때문에 remove_unused_columns=False 및 label_names=[\"labels\"]를 명시적으로 설정\n",
        "2. INT8 훈련에는 자동 캐스팅이 필요하기 때문에 Trainer에서 기본적으로 제공되는 predict_with_generate 호출을 사용할 수 없습니다. 자동 캐스팅이 자동으로 적용되지 않음\n",
        "3. 자동 캐스팅을 사용할 수 없으므로 Seq2SeqTrainer에 compute_metrics를 전달할 수 없음. 따라서 Trainer를 인스턴스화하는 동안 compute_metrics를 주석 처리해야 합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "zTQwzrWpCfzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
        "\n",
        "# This callback helps to save only the adapter weights and remove the base model weights.\n",
        "class SavePeftModelCallback(TrainerCallback):\n",
        "    def on_save(\n",
        "        self,\n",
        "        args: TrainingArguments,\n",
        "        state: TrainerState,\n",
        "        control: TrainerControl,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
        "\n",
        "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
        "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
        "\n",
        "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
        "        if os.path.exists(pytorch_model_path):\n",
        "            os.remove(pytorch_model_path)\n",
        "        return control\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=common_voice[\"train\"],\n",
        "    eval_dataset=common_voice[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    # compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    callbacks=[SavePeftModelCallback],\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ],
      "metadata": {
        "id": "UA4NRFR4ChER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "shqYZD0NCiFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our model is fine-tuned, we can push\n",
        "\n",
        "*   항목 추가\n",
        "*   항목 추가\n",
        "\n",
        "the model on to Hugging Face Hub, this will later help us directly infer the model from the model repo."
      ],
      "metadata": {
        "id": "7qL5wO52CkMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation and Inference"
      ],
      "metadata": {
        "id": "Pi2eDMnqCofB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On to the fun part, we've successfully fine-tuned our model. Now let's put it to test and calculate the WER on the `test` set.\n",
        "\n",
        "As with training, we do have a few caveats to pay attention to:\n",
        "1. Since we cannot use `predict_with_generate` function, we will hand roll our own eval loop with `torch.cuda.amp.autocast()` you can check it out below.\n",
        "2. Since the base model is frozen, PEFT model sometimes fails to recognise the language while decoding. To fix that, we force the starting tokens to mention the language we are transcribing. This is done via `forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"Marathi\", task=\"transcribe\")` and passing that too the `model.generate` call.\n",
        "\n",
        "That's it, let's get transcribing! 🔥\n"
      ],
      "metadata": {
        "id": "ZwEFCWVKCqWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer\n",
        "\n",
        "peft_model_id = \"reach-vb/whisper-large-v2-hindi-100steps\" # Use the same model ID as before.\n",
        "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\n",
        "    peft_config.base_model_name_or_path, load_in_8bit=True, device_map=\"auto\"\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "model.config.use_cache = True"
      ],
      "metadata": {
        "id": "jXaUINtDCrcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
        "\n",
        "eval_dataloader = DataLoader(common_voice[\"test\"], batch_size=8, collate_fn=data_collator)\n",
        "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n",
        "normalizer = BasicTextNormalizer()\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "normalized_predictions = []\n",
        "normalized_references = []\n",
        "\n",
        "model.eval()\n",
        "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
        "    with torch.cuda.amp.autocast():\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = (\n",
        "                model.generate(\n",
        "                    input_features=batch[\"input_features\"].to(\"cuda\"),\n",
        "                    forced_decoder_ids=forced_decoder_ids,\n",
        "                    max_new_tokens=255,\n",
        "                )\n",
        "                .cpu()\n",
        "                .numpy()\n",
        "            )\n",
        "            labels = batch[\"labels\"].cpu().numpy()\n",
        "            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
        "            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "            predictions.extend(decoded_preds)\n",
        "            references.extend(decoded_labels)\n",
        "            normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n",
        "            normalized_references.extend([normalizer(label).strip() for label in decoded_labels])\n",
        "        del generated_tokens, labels, batch\n",
        "    gc.collect()\n",
        "wer = 100 * metric.compute(predictions=predictions, references=references)\n",
        "normalized_wer = 100 * metric.compute(predictions=normalized_predictions, references=normalized_references)\n",
        "eval_metrics = {\"eval/wer\": wer, \"eval/normalized_wer\": normalized_wer}\n",
        "\n",
        "print(f\"{wer=} and {normalized_wer=}\")\n",
        "print(eval_metrics)"
      ],
      "metadata": {
        "id": "BxYB1BcbCsIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fin!\n",
        "\n",
        "If you made it all the way till the end then pat yourself on the back. Looking back, we learned how to train *any* Whisper checkpoint faster, cheaper and with negligible loss in WER.\n",
        "\n",
        "With PEFT, you can also go beyond Speech recognition and apply the same set of techniques to other pretrained models as well. Come check it out here: https://github.com/huggingface/peft 🤗\n",
        "\n",
        "Don't forget to tweet your results and tag us! [@huggingface](https://twitter.com/huggingface) and [@reach_vb](https://twitter.com/reach_vb) ❤️"
      ],
      "metadata": {
        "id": "shSu9FdvCtdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Whisper AI fine-tuning의 경량화 버전"
      ],
      "metadata": {
        "id": "ZzIomsfJDeOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "fasterWhisper_w_PEFT_finetune (quantization과 parameter efficient fine-tuning 기법)"
      ],
      "metadata": {
        "id": "ixJANAkIEfOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRA (Low Rank Adaptation) 와 PEFT (Paramater Efficient Fine Tuning)를 적용한 Larger Whisper fine-tuning ⚡️\n"
      ],
      "metadata": {
        "id": "dPzNx-FvDk_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 소비자 GPU의 VRAM이 8GB 미만인 환경에서도 full-finetuning과 유사한 성능을 제공함\n",
        "*  🤗 Transformers and PEFT 모델과 Common Voice 13.0 dataset를 사용하여 Whisper fine-tune하는 과정 설명함\n",
        "* PEFT와 bitsandbytes를 활용하여 무료 T4 GPU(16GB VRAM)를 사용하여 whisper-large-v2 체크포인트를 원활하게 학습"
      ],
      "metadata": {
        "id": "V7499VwMDwUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 왜 Parameter Efficient Fine Tuning [PEFT](https://github.com/huggingface/peft)를 사용해야 되는가?\n"
      ],
      "metadata": {
        "id": "bKotHLVADxtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 모델 사이즈 증가로 fine tuning하는 것이 계산 복잡성 증가와 메모리 사용량 증가\n",
        "    * 예를 들어, Whisper-large-v2 모델을 완전한 미세 조정을 위해 약 24GB의 GPU VRAM이 필요하며, 각 미세 조정된 모델은 약 7GB의 저장 공간을 필요함\n",
        "\n",
        "    * 제한적인 환경에서 bottleneck 발생하고 원하는 결과를 얻기 힘듦\n",
        "* PEFT\n",
        "    * 효과적으로 parameter를 줄여서 fine tuning 속도 개선\n",
        "    * 목적: 병목 현상을 해결\n",
        "    * 접근법(예: 저수준 적응): 사전 훈련된 모델의 대부분의 매개변수를 동결시키면서 추가 모델 매개변수의 일부만 미세 조정하여 계산 및 저장 비용 크게 줄임\n",
        "        * 대규모 모델의 전체 미세 조정 중 관찰되는 catastrophic forgetting 문제를 극복할 수 있음\n"
      ],
      "metadata": {
        "id": "TX3Hr4eYDzzU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LoRA가 무엇인가?"
      ],
      "metadata": {
        "id": "X6fmVz1gD1PY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* PEFT에서 여러 매개변수 효율적인 기술을 기본으로 제공\n",
        "    * 그 중 하나인 Low Rank Adaptation (LoRA)\n",
        "        * 사전 훈련된 모델 가중치를 동결하고 Transformer 아키텍처의 각 레이어에 훈련 가능한 랭크 분해 행렬을 삽입 (High Rank 즉 많은 연결이 되어 있는 것들보다 연결이 적은 Low Rank로 만들어서 계산량을 줄임)\n",
        "            * Downstream 작업에 대한 훈련 가능한 매개변수 수가 크게 감소"
      ],
      "metadata": {
        "id": "EaIZiLwGD3Hi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 통계로 보는 PEFT 효과"
      ],
      "metadata": {
        "id": "xTb93GTdD4UK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Full fine-tuning of Whisper-large-v2 checkpoint Vs. PEFT 적용 모델\n",
        "\n",
        "    1. GPU VRAM이 8GB 미만인 환경에서 16억 개의 매개변수를 가진 모델을 미세 조정 🤯\n",
        "    2. 훨씬 적은 수의 훈련 가능한 매개변수를 사용하여 거의 5배 더 큰 배치 크기를 사용 가능 📈\n",
        "    3. 생성된 체크포인트는 원본 모델의 크기의 1%인 약 60MB 🚀\n",
        "* 기존 🤗 transformers Whisper에서 변형이 많이 되지 않았음\n",
        "\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "q4qXzrZwD6AJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 환경설정"
      ],
      "metadata": {
        "id": "weCbrKZXEwuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Python package->Whisper 모델 fine tuning하기 위해 사용\n",
        "  * `datasets`:학습 데이터를 다운로드하고 준비\n",
        "  * `transformers`: Whisper 모델을 로드하고 훈련\n",
        "  * `librosa`: 오디오 파일을 전처리\n",
        "  * `evaluate` &  `jiwer`:모델의 성능을 평가\n",
        "  * `PEFT`, `bitsandbytes`, `accelerate`: LoRA로 모델과 fine-tuning"
      ],
      "metadata": {
        "id": "Pxt2i78zExte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets librosa evaluate jiwer gradio bitsandbytes==0.37 accelerate\n",
        "!pip install -q git+https://github.com/huggingface/peft.git@main"
      ],
      "metadata": {
        "id": "gIJNDn4AEy43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install typer==0.9.1"
      ],
      "metadata": {
        "id": "HNplKiMeE0Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* GPU 확보\n",
        "    * Google Colab Pro: V100 또는 P100 GPU가 할당\n",
        "* GPU 확보 방법\n",
        "    * 런타임 -> 런타임 유형 변경\n",
        "    * None -> GPU 변경\n",
        "* GPU 할당 확인"
      ],
      "metadata": {
        "id": "GIJ4GZPRE1eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "YyQlJSD_E2ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab에서 제공하는 GPU사용"
      ],
      "metadata": {
        "id": "Ap5Vjg_zE3rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "QF40Dh4uE4ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We strongly advise you to upload model checkpoints directly the [Hugging Face Hub](https://huggingface.co/)\n",
        "whilst training. The Hub provides:\n",
        "- Integrated version control: you can be sure that no model checkpoint is lost during training.\n",
        "- Tensorboard logs: track important metrics over the course of training.\n",
        "- Model cards: document what a model does and its intended use cases.\n",
        "- Community: an easy way to share and collaborate with the community!\n",
        "\n",
        "\n",
        "* 학습 중 모델 체크포인트를 직접 Hugging Face Hub에 업로드하는 것을 강력히 권장\n",
        " * Hub를 사용하면 다음과 같은 기능을 제공:\n",
        "\n",
        " 1. 통합된 버전 관리: 훈련 중에 어떠한 모델 체크포인트도 손실되지 않음\n",
        " 2. Tensorboard 로그: 훈련 과정에서 중요한 지표를 추적\n",
        " 3. 모델 카드: 모델이 무엇을 하고 의도된 사용 사례를 문서화\n",
        " 4. 커뮤니티: 커뮤니티와 쉽게 공유하고 협업\n",
        "\n",
        "* Hub에 연결\n",
        " * 프롬프트에서 Hub 인증 토큰을 입력:"
      ],
      "metadata": {
        "id": "tf3IIAeME5s3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "R9JK1YqnE7cX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Whisper 모델 checkpoint와 task 설정"
      ],
      "metadata": {
        "id": "6XJO6vi7E95o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"openai/whisper-large-v2\"\n",
        "task = \"transcribe\""
      ],
      "metadata": {
        "id": "l37TMk6qE_BL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "데이터셋 디테일을 설정 (언어)"
      ],
      "metadata": {
        "id": "L9dofpWCE__w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"mozilla-foundation/common_voice_13_0\"\n",
        "language = \"Korean\"\n",
        "language_abbr = \"ko\" # Short hand code for the language we want to fine-tune"
      ],
      "metadata": {
        "id": "Q0_VpqAqFAnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터셋 올리기\n"
      ],
      "metadata": {
        "id": "klhOGEvnFCq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Huggingface Dataset 사용\n",
        " * 적은 코드로 Common Voice의 데이터를 다운로드하고 준비\n",
        "\n",
        "* 확인 절차\n",
        " 1. Hugging Face Hub의 이용 약관을 수락했는지 확인:[mozilla-foundation/common_voice_13_0](https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0)\n",
        " 2. 데이터셋에 액세스하고 로컬로 데이터를 다운로드\n",
        "\n",
        "* 학습+검증 데이터셋/테스트 데이터셋 분리"
      ],
      "metadata": {
        "id": "rKL-c15cFF0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice[\"train\"] = load_dataset(dataset_name, language_abbr, split=\"train+validation\", use_auth_token=True)\n",
        "common_voice[\"test\"] = load_dataset(dataset_name, language_abbr, split=\"test\", use_auth_token=True)\n",
        "\n",
        "print(common_voice)"
      ],
      "metadata": {
        "id": "6ZozN_tfFMa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 일반적인 ASR(음성 인식) 데이터셋\n",
        "    * 입력 오디오 샘플(오디오)과 해당되는 텍스트(문장)만 제공\n",
        "* Common Voice\n",
        "    * ASR에는 필요하지 않은 악센트와 로케일과 같은 추가 메타데이터 정보가 포함\n",
        "    * 일반적인 용도로 사용하고 미세 조정을 고려하기 위해 메타데이터 정보 무시"
      ],
      "metadata": {
        "id": "_FHBG3yOFNVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = common_voice.remove_columns(\n",
        "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\", \"variant\"]\n",
        ")\n",
        "\n",
        "print(common_voice)"
      ],
      "metadata": {
        "id": "3O53cVAsFOPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 특성 추출기(Feature Extractor), 토크나이저(Tokenizer), 그리고 데이터준비\n"
      ],
      "metadata": {
        "id": "_ivtrPBAFPaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ASR 파이프라인 세 단계로 분해:\n",
        "\n",
        "1. Raw 오디오 입력을 전처리하는 특정 추출기\n",
        "2. 시퀀스 간 매핑을 수행하는 모델\n",
        "3. 모델 출력을 텍스트 형식으로 후처리하는 tokenizer\n",
        "\n",
        "\n",
        "* Whisper\n",
        "    * [WhisperFeatureExtractor](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperFeatureExtractor)와 [WhisperTokenizer](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperTokenizer)로 구성\n",
        "\n"
      ],
      "metadata": {
        "id": "iJXoUM3bFQ1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)"
      ],
      "metadata": {
        "id": "j00MN9qwFRxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperTokenizer\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)"
      ],
      "metadata": {
        "id": "01ArF8lCFSbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* WhisperProcessor 클라스\n",
        "    * 특성 추출기와 토크나이저를 사용하기 위해 두 가지를 모두 합칩\n",
        "    * 필요에 따라 오디오 입력 및 모델 예측에 사용 가능\n",
        "\n",
        "* 학습 중에 두 개의 객체만 추적 필요: 프로세서와 모델"
      ],
      "metadata": {
        "id": "JHEDUSrzFUSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
      ],
      "metadata": {
        "id": "vE4DXwpaFVJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 데이터 준비\n",
        "\n"
      ],
      "metadata": {
        "id": "SSUPah6yFWR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common Voice 데이터셋의 첫 번째 예제를 출력하여 데이터의 형식을 살펴봄"
      ],
      "metadata": {
        "id": "N7ub4GcLFYHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(common_voice[\"train\"][0])"
      ],
      "metadata": {
        "id": "La1zJTYRFbmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Whisper 모델 샘플링\n",
        "    * 입력 오디오는 48 kHz 새플링\n",
        "    * Whisper feature extractor에 전달하기 위해서 16 kHz로 다운샘플 진행\n",
        "* 샘플링 속도 설정\n",
        "    * Dataset의 [`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column) 방법 사용: 오디오 입력을 올바른 샘플링 속도로 설정\n",
        "    * 오디오를 변경하는 것이 아니라 오디오 샘플을 실시간으로 받을 수 있도록 함\n"
      ],
      "metadata": {
        "id": "3X0TJCD8FegP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Audio\n",
        "\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ],
      "metadata": {
        "id": "W8c-WIpaFfvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Common Voice 데이터셋에서 첫 번째 오디오 샘플을 다시로드하면 원하는 샘플링 속도로 다시 샘플링"
      ],
      "metadata": {
        "id": "BMV7Af7SFh-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(common_voice[\"train\"][0])"
      ],
      "metadata": {
        "id": "R8QA_OV1Fi2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* 모델에 맞게 데이터를 준비하는 함수:\n",
        "\n",
        "1. batch[\"audio\"]를 호출하여 오디오 데이터를 로드하고 다시 샘플링. 🤗 Datasets는 필요한 모든 재샘플링 작업을 실시간으로 수행\n",
        "3. Feature extractor를 사용하여 1차원 오디오 배열에서 로그 멜 스펙트로그램 입력 특성을 계산\n",
        "3. Tokenizer를 사용하여 transcripts를 레이블 ids로 인코딩\n"
      ],
      "metadata": {
        "id": "R1YL5hcvFkU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids\n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch"
      ],
      "metadata": {
        "id": "bHnrXcSoFltS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 데이터 준비 함수\n",
        "    * 데이터셋의 .map 메서드를 사용하여 모든 학습 예제에 적용 가능\n",
        "    * num_proc: 몇 개의 CPU 코어를 사용할 지를 지정,num_proc를 1보다 크게 설정하면 다중 처리가 활성화 (다중 처리로 .map 메서드가 중단되는 경우 num_proc=1로 설정하고 데이터셋을 순차적으로 처리)\n",
        "\n",
        "* Dataset의 사이즈에 따라서 20~30 분 정도 걸림\n"
      ],
      "metadata": {
        "id": "qyenTQcQFmz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2)"
      ],
      "metadata": {
        "id": "QZx_aOpJFoO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice[\"train\"]"
      ],
      "metadata": {
        "id": "tr7R7OtcFpeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 및 검증\n"
      ],
      "metadata": {
        "id": "ZvXDlAlVFq1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* 훈련 파이프라인\n",
        "* [🤗 Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer)가 대부분의 작업을 처리:\n",
        "\n",
        "\n",
        "1. 데이터 collator 정의: 데이터 콜레이터는 우리가 전처리한 데이터를 가져와 모델에 사용할 수 있는 PyTorch 텐서로 준비\n",
        "2. 평가 지표: 평가 중에는 모델을 글자 오류율  [word error rate (CER)](https://huggingface.co/metrics/cer)지표를 사용하여 평가\n",
        "3. 사전 훈련된 체크포인트 load: 사전 훈련된 체크포인트를 로드하고 훈련을 위해 올바르게 구성\n",
        "4. 훈련 구성 정의: 🤗 Trainer가 훈련 스케줄을 정의에 사용\n",
        "\n",
        "* 미세 조정한 후에는 테스트 데이터에서 모델을 평가하여 한국어 음성을 올바르게 transcribe\n"
      ],
      "metadata": {
        "id": "Oun5Wnc2FsPH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Collator 정의\n"
      ],
      "metadata": {
        "id": "KBITeAvgFt44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* 시퀀스-투-시퀀스 음성 모델의 데이터 콜레이터\n",
        "    * Input_features와 labels를 독립적으로 처리 차별화\n",
        "    \n",
        "    - Input_features: feature extractor에 의해 처리\n",
        "    - labels: tokenizer에 의해 처리\n",
        "\n",
        "* Input_features는 이미 30초로 패딩되어 있고 특성 추출기에 의해 고정된 차원의 로그 멜 스펙트로그램으로 변환. 따라서 우리가 해야 할 일은 input_features를 배치 처리된 PyTorch 텐서로 변환\n",
        "\n",
        "* labels는 패딩되지 않음 먼저 배치 내에서 최대 길이에 맞게 시퀀스를 패딩하고, tokenizer의 .pad 메서드를 사용하여 시퀀스를 패딩 패딩 토큰은 손실을 계산할 때 고려되지 않도록 -100으로 대체. 그런 다음 레이블 시퀀스의 시작에서 BOS 토큰을 잘라서 훈련 중에 나중에 이를 추가\n",
        "\n",
        "* 이전에 정의한 WhisperProcessor를 활용하여 특성 추출기 및 토크나이저 작업을 모두 수행 가능"
      ],
      "metadata": {
        "id": "umBjrMi9FviZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "RPDj2653Fwt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data collator 초기화 진행"
      ],
      "metadata": {
        "id": "vnUqIKCRFyAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ],
      "metadata": {
        "id": "2BeOoSfAFyxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 평가 지표"
      ],
      "metadata": {
        "id": "Rc1Ovn_iFzso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* ASR 시스템을 평가하기 위한 '사실상의' 지표인 한 단어 오류율(CER) 메트릭을 사용\n",
        "* 더 많은 정보는 [문서](https://huggingface.co/metrics/cer)를 참조. 우리는 🤗 Evaluate에서 CER 메트릭을 로드"
      ],
      "metadata": {
        "id": "76KMFfvfF0td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"cer\")"
      ],
      "metadata": {
        "id": "D3of2K85F1Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pre-trained 모델 로드"
      ],
      "metadata": {
        "id": "TklAg4VeF2Wt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 사전 훈련된 Whisper 체크포인트를 로드\n",
        "    * 이 작업은 🤗 Transformers를 사용하여 매우 간단\n",
        "\n",
        "\n",
        "\n",
        "* 모델의 메모리 사용량을 줄이기 위해 모델을 8비트로         \n",
        "    * 모델을 1/4 정밀도(32비트와 비교했을 때)로 양자화하여 성능 손실을 최소화 [here](https://huggingface.co/blog/hf-bitsandbytes-integration)"
      ],
      "metadata": {
        "id": "12EdQdpuF3yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, load_in_8bit=True, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "3Acl29i1F42G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 모델의 후처리"
      ],
      "metadata": {
        "id": "URrUdimqF6TP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. 훈련을 가능하게 하기 위해 8비트 모델에 몇 가지 후처리 단계를 적용\n",
        "2. 모델 레이어를 동결, 훈련과 모델의 안정성을 위해 레이어 정규화와 출력 레이어를 float32로 캐스팅\n",
        "\n",
        "(모델 안정성과 layer normalization 분석, float32로 캐스팅 하는 이유)"
      ],
      "metadata": {
        "id": "oKj9unKWF7c6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "id": "qChHa0ptF8eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model = prepare_model_for_kbit_training(model, output_embedding_layer_name=\"proj_out\")"
      ],
      "metadata": {
        "id": "c6CieDF-F9TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Whisper 모델은 인코더에 컨볼루션 레이어를 사용하기 때문에 체크포인팅은 grad 연산을 비활성. 이를 피하기 위해 입력을 특별히 trainable하게 만들어야 합니다.\n"
      ],
      "metadata": {
        "id": "tNpShzrUF-Pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_inputs_require_grad(module, input, output):\n",
        "    output.requires_grad_(True)\n",
        "\n",
        "model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)"
      ],
      "metadata": {
        "id": "D_XQsrQ6F_GH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Low-rank adapters (LoRA)를 모델에 적용\n"
      ],
      "metadata": {
        "id": "SA03MdIkGAS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* peft에서 get_peft_model 유틸리티 함수를 사용하여 PeftModel을 로드하고 저희가 저차원 어댑터(LoRA)를 사용할 것임을 지정\n"
      ],
      "metadata": {
        "id": "PvwkztSoGBiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "dwMYIPnYGCyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1%**의 학습 parameter를 사용하였고 **Parameter-Efficient Fine-Tuning**를 적용\n"
      ],
      "metadata": {
        "id": "mkOvUzL5GDw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 훈련 구성 정의"
      ],
      "metadata": {
        "id": "jJNqmo2lGE1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "마지막 단계에서는 훈련과 관련된 모든 매개변수를 정의 훈련 인자에 대한 자세한 내용은 해당 문서를 참조 Seq2SeqTrainingArguments [docs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments)\n"
      ],
      "metadata": {
        "id": "CGwnUXB4GHhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"reach-vb/test\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-3,\n",
        "    warmup_steps=50,\n",
        "    num_train_epochs=1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    fp16=True,\n",
        "    per_device_eval_batch_size=8,\n",
        "    generation_max_length=128,\n",
        "    logging_steps=100,\n",
        "    max_steps=100, # only for testing purposes, remove this from your final run :)\n",
        "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
        "    label_names=[\"labels\"],  # same reason as above\n",
        ")"
      ],
      "metadata": {
        "id": "7qPc07lFGG9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* PEFT를 사용하여 모델을 미세 조정하는 것에는 몇 가지 주의가 필요\n",
        "\n",
        "1. PeftModel의 forward가 기본 모델의 forward의 시그니처를 상속하지 않기 때문에 remove_unused_columns=False 및 label_names=[\"labels\"]를 명시적으로 설정\n",
        "2. INT8 훈련에는 자동 캐스팅이 필요하기 때문에 Trainer에서 기본적으로 제공되는 predict_with_generate 호출을 사용할 수 없습니다. 자동 캐스팅이 자동으로 적용되지 않음\n",
        "3. 자동 캐스팅을 사용할 수 없으므로 Seq2SeqTrainer에 compute_metrics를 전달할 수 없음. 따라서 Trainer를 인스턴스화하는 동안 compute_metrics를 주석 처리해야 합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "jd9TNUakGJhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
        "\n",
        "# This callback helps to save only the adapter weights and remove the base model weights.\n",
        "class SavePeftModelCallback(TrainerCallback):\n",
        "    def on_save(\n",
        "        self,\n",
        "        args: TrainingArguments,\n",
        "        state: TrainerState,\n",
        "        control: TrainerControl,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
        "\n",
        "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
        "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
        "\n",
        "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
        "        if os.path.exists(pytorch_model_path):\n",
        "            os.remove(pytorch_model_path)\n",
        "        return control\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=common_voice[\"train\"],\n",
        "    eval_dataset=common_voice[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    # compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    callbacks=[SavePeftModelCallback],\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ],
      "metadata": {
        "id": "mlxRmXiGGKs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "jc2hSblfGL9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our model is fine-tuned, we can push the model on to Hugging Face Hub, this will later help us directly infer the model from the model repo."
      ],
      "metadata": {
        "id": "N_y3dUucGM6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_id = \"reach-vb/whisper-large-v2-hindi-100steps\"\n",
        "model.push_to_hub(peft_model_id)"
      ],
      "metadata": {
        "id": "l2SCSgBQGOBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation and Inference"
      ],
      "metadata": {
        "id": "K79J0U3EGPHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On to the fun part, we've successfully fine-tuned our model. Now let's put it to test and calculate the WER on the `test` set.\n",
        "\n",
        "As with training, we do have a few caveats to pay attention to:\n",
        "1. Since we cannot use `predict_with_generate` function, we will hand roll our own eval loop with `torch.cuda.amp.autocast()` you can check it out below.\n",
        "2. Since the base model is frozen, PEFT model sometimes fails to recognise the language while decoding. To fix that, we force the starting tokens to mention the language we are transcribing. This is done via `forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"Marathi\", task=\"transcribe\")` and passing that too the `model.generate` call.\n",
        "\n",
        "That's it, let's get transcribing! 🔥\n"
      ],
      "metadata": {
        "id": "-q4B_OCYGQev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer\n",
        "\n",
        "peft_model_id = \"reach-vb/whisper-large-v2-hindi-100steps\" # Use the same model ID as before.\n",
        "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\n",
        "    peft_config.base_model_name_or_path, load_in_8bit=True, device_map=\"auto\"\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "model.config.use_cache = True"
      ],
      "metadata": {
        "id": "s2kHJxk1GTWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
        "\n",
        "eval_dataloader = DataLoader(common_voice[\"test\"], batch_size=8, collate_fn=data_collator)\n",
        "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n",
        "normalizer = BasicTextNormalizer()\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "normalized_predictions = []\n",
        "normalized_references = []\n",
        "\n",
        "model.eval()\n",
        "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
        "    with torch.cuda.amp.autocast():\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = (\n",
        "                model.generate(\n",
        "                    input_features=batch[\"input_features\"].to(\"cuda\"),\n",
        "                    forced_decoder_ids=forced_decoder_ids,\n",
        "                    max_new_tokens=255,\n",
        "                )\n",
        "                .cpu()\n",
        "                .numpy()\n",
        "            )\n",
        "            labels = batch[\"labels\"].cpu().numpy()\n",
        "            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
        "            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "            predictions.extend(decoded_preds)\n",
        "            references.extend(decoded_labels)\n",
        "            normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n",
        "            normalized_references.extend([normalizer(label).strip() for label in decoded_labels])\n",
        "        del generated_tokens, labels, batch\n",
        "    gc.collect()\n",
        "wer = 100 * metric.compute(predictions=predictions, references=references)\n",
        "normalized_wer = 100 * metric.compute(predictions=normalized_predictions, references=normalized_references)\n",
        "eval_metrics = {\"eval/wer\": wer, \"eval/normalized_wer\": normalized_wer}\n",
        "\n",
        "print(f\"{wer=} and {normalized_wer=}\")\n",
        "print(eval_metrics)"
      ],
      "metadata": {
        "id": "Y55VK-B0GUIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fin!\n"
      ],
      "metadata": {
        "id": "9Z9qEpEGGS13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "If you made it all the way till the end then pat yourself on the back. Looking back, we learned how to train *any* Whisper checkpoint faster, cheaper and with negligible loss in WER.\n",
        "\n",
        "With PEFT, you can also go beyond Speech recognition and apply the same set of techniques to other pretrained models as well. Come check it out here: https://github.com/huggingface/peft 🤗\n",
        "\n",
        "Don't forget to tweet your results and tag us! [@huggingface](https://twitter.com/huggingface) and [@reach_vb](https://twitter.com/reach_vb) ❤️"
      ],
      "metadata": {
        "id": "UkZWOMr3GXXJ"
      }
    }
  ]
}